2021-10-26 13:38:19 [scrapy.extensions.telnet] INFO: Telnet Password: 23cd651cf139afb9
2021-10-26 13:38:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:38:20 [werkzeug] WARNING:  * Debugger is active!
2021-10-26 13:38:20 [werkzeug] INFO:  * Debugger PIN: 835-249-238
2021-10-26 13:38:20 [werkzeug] INFO:  * Running on http://0.0.0.0:10288/ (Press CTRL+C to quit)
2021-10-26 13:38:29 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:38:29] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:38:29 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:38:29] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2021-10-26 13:39:44 [werkzeug] INFO:  * Detected change in 'E:\\workspace\\TextProcessor\\TextProcessorScrapy\\middlewares.py', reloading
2021-10-26 13:39:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:44 [scrapy.extensions.telnet] INFO: Telnet Password: db2621cd201e0667
2021-10-26 13:39:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:44 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:44 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 89, in crawl
    self.engine = self._create_engine()
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 103, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\misc.py", line 50, in load_object
    mod = import_module(module)
  File "D:\Anaconda\envs\HS\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 23, in <module>
    class TextCrawlSpiderMiddleware:
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\serving.py", line 1050, in run_simple
    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 337, in run_with_reloader
    reloader.run()
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 213, in run
    self.trigger_reload(filename)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 189, in trigger_reload
    sys.exit(3)
SystemExit: 3
2021-10-26 13:39:44 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.WikiPipeline']
2021-10-26 13:39:44 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:44 [wiki] INFO: Spider opened: wiki
2021-10-26 13:39:44 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:44 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-10-26 13:39:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:44 [scrapy.extensions.telnet] INFO: Telnet Password: 0372ba1251bd8802
2021-10-26 13:39:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:45 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}
2021-10-26 13:39:45 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1
2021-10-26 13:39:45 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session HTTP/1.1" 200 783
2021-10-26 13:39:45 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:39:45 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/timeouts {"implicit": 1000}
2021-10-26 13:39:45 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/timeouts HTTP/1.1" 200 14
2021-10-26 13:39:45 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.BaiduPipeline']
2021-10-26 13:39:45 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:45 [baidu] INFO: Spider opened: baidu
2021-10-26 13:39:45 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:45 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2021-10-26 13:39:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:45 [scrapy.extensions.telnet] INFO: Telnet Password: ac3f83a9f230a7a1
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.TiexuePipeline']
2021-10-26 13:39:45 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:45 [tiexue] INFO: Spider opened: tiexue
2021-10-26 13:39:45 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:45 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2021-10-26 13:39:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:45 [scrapy.extensions.telnet] INFO: Telnet Password: 4531feeaa5901ba8
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:45 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.AiaaPipeline']
2021-10-26 13:39:45 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:45 [aiaa] INFO: Spider opened: aiaa
2021-10-26 13:39:45 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:45 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2021-10-26 13:39:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:46 [scrapy.extensions.telnet] INFO: Telnet Password: 1fc6b30aea3d528a
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.HsNasaPipeline']
2021-10-26 13:39:46 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:46 [nasa] INFO: Spider opened: nasa
2021-10-26 13:39:46 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:46 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2021-10-26 13:39:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:46 [scrapy.extensions.telnet] INFO: Telnet Password: b57c070ded787a96
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.WikiPipeline']
2021-10-26 13:39:46 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:46 [wiki] INFO: Spider opened: wiki
2021-10-26 13:39:46 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:46 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028
2021-10-26 13:39:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:46 [scrapy.extensions.telnet] INFO: Telnet Password: 5c3307c7dbdc13a5
2021-10-26 13:39:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:46 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:2601/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}
2021-10-26 13:39:46 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1
2021-10-26 13:39:47 [urllib3.connectionpool] DEBUG: http://127.0.0.1:2601 "POST /session HTTP/1.1" 200 783
2021-10-26 13:39:47 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:39:47 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:2601/session/0aa9e77ba0ce873c39ca3851271c939a/timeouts {"implicit": 1000}
2021-10-26 13:39:47 [urllib3.connectionpool] DEBUG: http://127.0.0.1:2601 "POST /session/0aa9e77ba0ce873c39ca3851271c939a/timeouts HTTP/1.1" 200 14
2021-10-26 13:39:47 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.BaiduPipeline']
2021-10-26 13:39:47 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:47 [baidu] INFO: Spider opened: baidu
2021-10-26 13:39:47 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:47 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029
2021-10-26 13:39:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:47 [scrapy.extensions.telnet] INFO: Telnet Password: bf37dc0558c60d47
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.TiexuePipeline']
2021-10-26 13:39:47 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:47 [tiexue] INFO: Spider opened: tiexue
2021-10-26 13:39:47 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:47 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6030
2021-10-26 13:39:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_11.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:39:47 [scrapy.extensions.telnet] INFO: Telnet Password: d76d62edb39d9eeb
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:39:47 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.AiaaPipeline']
2021-10-26 13:39:47 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:39:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:39:47 [aiaa] INFO: Spider opened: aiaa
2021-10-26 13:39:47 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:39:47 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:39:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6031
2021-10-26 13:39:47 [TextProcessorScrapy.spiders.Aiaa] INFO: Aiaa Spider Starting!
2021-10-26 13:39:47 [TextProcessorScrapy.spiders.Aiaa] INFO: Aiaa Spider Starting!
2021-10-26 13:40:08 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:08] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:08 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:08] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:09 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:09] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:40:10 [werkzeug] INFO: 127.0.0.1 - - [26/Oct/2021 13:40:10] "[33mGET / HTTP/1.1[0m" 404 -
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E6%9E%AA%E6%A6%B4%E5%BC%B9&ns0=1> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://nasasearch.nasa.gov/search?query=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&affiliate=nasa&utf8=%E2%9C%93> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&ns0=1> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&sortBy=Earliest&startPage=0&pageSize=20&> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E6%9E%AA%E6%A6%B4%E5%BC%B9&sortBy=Earliest&startPage=0&pageSize=20&> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.baidu.com/baidu?word=%E6%9E%AA%E6%A6%B4%E5%BC%B9&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on> from <GET https://baidu.com/baidu?word=%E6%9E%AA%E6%A6%B4%E5%BC%B9&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on>
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://baike.baidu.com/item/%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0/5794> from <GET https://baike.baidu.com/item/%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0>
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.baidu.com/baidu?word=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on> from <GET https://baidu.com/baidu?word=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on>
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E6%9E%AA%E6%A6%B4%E5%BC%B9&ns0=1> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://nasasearch.nasa.gov/search?query=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&affiliate=nasa&utf8=%E2%9C%93> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&ns0=1> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&sortBy=Earliest&startPage=0&pageSize=20&> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E6%9E%AA%E6%A6%B4%E5%BC%B9&sortBy=Earliest&startPage=0&pageSize=20&> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://baike.baidu.com/item/%E6%9E%AA%E6%A6%B4%E5%BC%B9> (referer: None)
2021-10-26 13:41:00 [py.warnings] WARNING: C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\scraper.py:148: UserWarning: The "BaidubaikeSpider.parse" method is a generator and includes a "return" statement with a value different than None. This could lead to unexpected behaviour. Please see https://docs.python.org/3/reference/simple_stmts.html#the-return-statement for details about the semantics of the "return" statement within generators
  warn_on_generator_with_return_value(spider, callback)

2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E6%9E%AA%E6%A6%B4%E5%BC%B9&ns0=1> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://nasasearch.nasa.gov/search?query=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&affiliate=nasa&utf8=%E2%9C%93> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E6%9E%AA%E6%A6%B4%E5%BC%B9&sortBy=Earliest&startPage=0&pageSize=20&> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://arc.aiaa.org/action/doSearch?AllField=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&sortBy=Earliest&startPage=0&pageSize=20&> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&ns0=1> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:00 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/url {"url": "https://baike.baidu.com/item/%E6%9E%AA%E6%A6%B4%E5%BC%B9"}
2021-10-26 13:41:01 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/url HTTP/1.1" 200 14
2021-10-26 13:41:01 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:01 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "return document.body.scrollHeight;", "args": []}
2021-10-26 13:41:01 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:01 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:03 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "window.scrollBy(0,1500)", "args": []}
2021-10-26 13:41:03 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:03 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:05 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "window.scrollBy(0,1500)", "args": []}
2021-10-26 13:41:05 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:05 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:07 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "window.scrollBy(0,1500)", "args": []}
2021-10-26 13:41:07 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:07 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "window.scrollBy(0,1500)", "args": []}
2021-10-26 13:41:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "return document.body.scrollHeight;", "args": []}
2021-10-26 13:41:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 14
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/elements {"using": "xpath", "value": "//img[contains(@alt, '\u67aa\u69b4\u5f39')]"}
2021-10-26 13:41:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/elements HTTP/1.1" 200 90
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:1970/session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync {"script": "return (function(){return function(){var d=this;function f(a){return\"string\"==typeof a};function h(a,b){this.code=a;this.a=l[a]||m;this.message=b||\"\";a=this.a.replace(/((?:^|\\s+)[a-z])/g,function(a){return a.toUpperCase().replace(/^[\\s\\xa0]+/g,\"\")});b=a.length-5;if(0>b||a.indexOf(\"Error\",b)!=b)a+=\"Error\";this.name=a;a=Error(this.message);a.name=this.name;this.stack=a.stack||\"\"}\n(function(){var a=Error;function b(){}b.prototype=a.prototype;h.b=a.prototype;h.prototype=new b;h.prototype.constructor=h;h.a=function(b,c,g){for(var e=Array(arguments.length-2),k=2;k<arguments.length;k++)e[k-2]=arguments[k];return a.prototype[c].apply(b,e)}})();var m=\"unknown error\",l={15:\"element not selectable\",11:\"element not visible\"};l[31]=m;l[30]=m;l[24]=\"invalid cookie domain\";l[29]=\"invalid element coordinates\";l[12]=\"invalid element state\";l[32]=\"invalid selector\";l[51]=\"invalid selector\";\nl[52]=\"invalid selector\";l[17]=\"javascript error\";l[405]=\"unsupported operation\";l[34]=\"move target out of bounds\";l[27]=\"no such alert\";l[7]=\"no such element\";l[8]=\"no such frame\";l[23]=\"no such window\";l[28]=\"script timeout\";l[33]=\"session not created\";l[10]=\"stale element reference\";l[21]=\"timeout\";l[25]=\"unable to set cookie\";l[26]=\"unexpected alert open\";l[13]=m;l[9]=\"unknown command\";h.prototype.toString=function(){return this.name+\": \"+this.message};var n;a:{var p=d.navigator;if(p){var q=p.userAgent;if(q){n=q;break a}}n=\"\"}function r(a){return-1!=n.indexOf(a)};function t(a,b){for(var e=a.length,c=f(a)?a.split(\"\"):a,g=0;g<e;g++)g in c&&b.call(void 0,c[g],g,a)};function v(){return r(\"iPhone\")&&!r(\"iPod\")&&!r(\"iPad\")};function w(){return(r(\"Chrome\")||r(\"CriOS\"))&&!r(\"Edge\")};var x=r(\"Opera\"),y=r(\"Trident\")||r(\"MSIE\"),z=r(\"Edge\"),A=r(\"Gecko\")&&!(-1!=n.toLowerCase().indexOf(\"webkit\")&&!r(\"Edge\"))&&!(r(\"Trident\")||r(\"MSIE\"))&&!r(\"Edge\"),aa=-1!=n.toLowerCase().indexOf(\"webkit\")&&!r(\"Edge\");function B(){var a=d.document;return a?a.documentMode:void 0}var C;\na:{var D=\"\",E=function(){var a=n;if(A)return/rv\\:([^\\);]+)(\\)|;)/.exec(a);if(z)return/Edge\\/([\\d\\.]+)/.exec(a);if(y)return/\\b(?:MSIE|rv)[: ]([^\\);]+)(\\)|;)/.exec(a);if(aa)return/WebKit\\/(\\S+)/.exec(a);if(x)return/(?:Version)[ \\/]?(\\S+)/.exec(a)}();E&&(D=E?E[1]:\"\");if(y){var F=B();if(null!=F&&F>parseFloat(D)){C=String(F);break a}}C=D}var G;var H=d.document;G=H&&y?B()||(\"CSS1Compat\"==H.compatMode?parseInt(C,10):5):void 0;var ba=r(\"Firefox\"),ca=v()||r(\"iPod\"),da=r(\"iPad\"),I=r(\"Android\")&&!(w()||r(\"Firefox\")||r(\"Opera\")||r(\"Silk\")),ea=w(),J=r(\"Safari\")&&!(w()||r(\"Coast\")||r(\"Opera\")||r(\"Edge\")||r(\"Silk\")||r(\"Android\"))&&!(v()||r(\"iPad\")||r(\"iPod\"));function K(a){return(a=a.exec(n))?a[1]:\"\"}(function(){if(ba)return K(/Firefox\\/([0-9.]+)/);if(y||z||x)return C;if(ea)return v()||r(\"iPad\")||r(\"iPod\")?K(/CriOS\\/([0-9.]+)/):K(/Chrome\\/([0-9.]+)/);if(J&&!(v()||r(\"iPad\")||r(\"iPod\")))return K(/Version\\/([0-9.]+)/);if(ca||da){var a=/Version\\/(\\S+).*Mobile\\/(\\S+)/.exec(n);if(a)return a[1]+\".\"+a[2]}else if(I)return(a=K(/Android\\s+([0-9.]+)/))?a:K(/Version\\/([0-9.]+)/);return\"\"})();var L,M=function(){if(!A)return!1;var a=d.Components;if(!a)return!1;try{if(!a.classes)return!1}catch(g){return!1}var b=a.classes,a=a.interfaces,e=b[\"@mozilla.org/xpcom/version-comparator;1\"].getService(a.nsIVersionComparator),c=b[\"@mozilla.org/xre/app-info;1\"].getService(a.nsIXULAppInfo).version;L=function(a){e.compare(c,\"\"+a)};return!0}(),N=y&&!(8<=Number(G)),fa=y&&!(9<=Number(G));I&&M&&L(2.3);I&&M&&L(4);J&&M&&L(6);var ga={SCRIPT:1,STYLE:1,HEAD:1,IFRAME:1,OBJECT:1},O={IMG:\" \",BR:\"\\n\"};function P(a,b,e){if(!(a.nodeName in ga))if(3==a.nodeType)e?b.push(String(a.nodeValue).replace(/(\\r\\n|\\r|\\n)/g,\"\")):b.push(a.nodeValue);else if(a.nodeName in O)b.push(O[a.nodeName]);else for(a=a.firstChild;a;)P(a,b,e),a=a.nextSibling};function Q(a,b){b=b.toLowerCase();return\"style\"==b?ha(a.style.cssText):N&&\"value\"==b&&R(a,\"INPUT\")?a.value:fa&&!0===a[b]?String(a.getAttribute(b)):(a=a.getAttributeNode(b))&&a.specified?a.value:null}var ia=/[;]+(?=(?:(?:[^\"]*\"){2})*[^\"]*$)(?=(?:(?:[^']*'){2})*[^']*$)(?=(?:[^()]*\\([^()]*\\))*[^()]*$)/;\nfunction ha(a){var b=[];t(a.split(ia),function(a){var c=a.indexOf(\":\");0<c&&(a=[a.slice(0,c),a.slice(c+1)],2==a.length&&b.push(a[0].toLowerCase(),\":\",a[1],\";\"))});b=b.join(\"\");return b=\";\"==b.charAt(b.length-1)?b:b+\";\"}function S(a,b){N&&\"value\"==b&&R(a,\"OPTION\")&&null===Q(a,\"value\")?(b=[],P(a,b,!1),a=b.join(\"\")):a=a[b];return a}function R(a,b){b&&\"string\"!==typeof b&&(b=b.toString());return!!a&&1==a.nodeType&&(!b||a.tagName.toUpperCase()==b)}\nfunction T(a){return R(a,\"OPTION\")?!0:R(a,\"INPUT\")?(a=a.type.toLowerCase(),\"checkbox\"==a||\"radio\"==a):!1};var ja={\"class\":\"className\",readonly:\"readOnly\"},U=\"allowfullscreen allowpaymentrequest allowusermedia async autofocus autoplay checked compact complete controls declare default defaultchecked defaultselected defer disabled ended formnovalidate hidden indeterminate iscontenteditable ismap itemscope loop multiple muted nohref nomodule noresize noshade novalidate nowrap open paused playsinline pubdate readonly required reversed scoped seamless seeking selected truespeed typemustmatch willvalidate\".split(\" \");function V(a,b){var e=null,c=b.toLowerCase();if(\"style\"==c)return(e=a.style)&&!f(e)&&(e=e.cssText),e;if((\"selected\"==c||\"checked\"==c)&&T(a)){if(!T(a))throw new h(15,\"Element is not selectable\");b=\"selected\";e=a.type&&a.type.toLowerCase();if(\"checkbox\"==e||\"radio\"==e)b=\"checked\";return S(a,b)?\"true\":null}var g=R(a,\"A\");if(R(a,\"IMG\")&&\"src\"==c||g&&\"href\"==c)return(e=Q(a,c))&&(e=S(a,c)),e;if(\"spellcheck\"==c){e=Q(a,c);if(null!==e){if(\"false\"==e.toLowerCase())return\"false\";if(\"true\"==e.toLowerCase())return\"true\"}return S(a,\nc)+\"\"}g=ja[b]||b;a:if(f(U))c=f(c)&&1==c.length?U.indexOf(c,0):-1;else{for(var u=0;u<U.length;u++)if(u in U&&U[u]===c){c=u;break a}c=-1}if(0<=c)return(e=null!==Q(a,b)||S(a,g))?\"true\":null;try{var k=S(a,g)}catch(ka){}(c=null==k)||(c=typeof k,c=\"object\"==c&&null!=k||\"function\"==c);c?e=Q(a,b):e=k;return null!=e?e.toString():null}var W=[\"_\"],X=d;W[0]in X||!X.execScript||X.execScript(\"var \"+W[0]);\nfor(var Y;W.length&&(Y=W.shift());){var Z;if(Z=!W.length)Z=void 0!==V;Z?X[Y]=V:X[Y]&&X[Y]!==Object.prototype[Y]?X=X[Y]:X=X[Y]={}};; return this._.apply(null,arguments);}.apply({navigator:typeof window!='undefined'?window.navigator:null,document:typeof window!='undefined'?window.document:null}, arguments);}\n).apply(null, arguments);", "args": [{"ELEMENT": "d82a6e49-4ed6-4d0e-9e9c-a2ece1709115", "element-6066-11e4-a52e-4f735466cecf": "d82a6e49-4ed6-4d0e-9e9c-a2ece1709115"}, "src"]}
2021-10-26 13:41:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:1970 "POST /session/93e17929f4949fcab1bf1b072b96b8d6/execute/sync HTTP/1.1" 200 147
2021-10-26 13:41:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:41:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://baike.baidu.com/item/%E6%9E%AA%E6%A6%B4%E5%BC%B9>
{'attributes': {'img_url': {'图片1': 'https://bkimg.cdn.bcebos.com/pic/14ce36d3d539b60038fd63c6e950352ac65cb78b?x-bce-process=image/resize,m_lfit,w_440,limit_1/format,f_auto'},
                '中文名': '枪榴弹',
                '作用': '杀伤野战工事',
                '使用兵种': '步兵',
                '分类': '主要分为杀伤型和反装甲型',
                '特点': '提高了步兵作战能力'},
 'content': '枪榴弹是步兵近距离上使用的点面杀伤武器，主要用于杀伤有生目标，摧毁各种轻型装甲目标、永久火力点等野战工事。因此，枪榴弹的装备使用大大提高了步兵在现代战场上对付点、面目标和反装甲的作战能力。[1]\xa0'
            '枪榴弹的种类较多。按用途，分为杀伤枪榴弹、破甲枪榴弹、多用途枪榴弹、燃烧枪榴弹、照明枪榴弹、发烟枪榴弹、化学枪榴弹、演习训练枪榴弹等；按发射装置，分为尾管式枪榴弹、尾杆式枪榴弹、全入式枪榴弹、环翼式枪榴弹和弹筒合一式枪榴弹等。[2]\xa0'
            '由战斗部、引信和弹尾部件组成。战斗部的结构与手榴弹相似，形状为圆柱形，内装炸药、化学药剂和其他元件，如破甲枪榴弹的药型罩等。引信有延期时间引信、触发引信、近炸引信和其他引信，使战斗部能在最有利时机作用。弹尾部件有尾管和尾杆两种。发射时尾管套在枪榴弹发射具上，承载发射推力并赋予枪榴弹一定的射向，尾管外有翼片，其作用是保持枪榴弹的飞行稳定。尾杆是早期枪榴弹采用的部件，大多为木杆，杆上刻有射程标记，发射时将尾杆插入筒形发射具，以插入的深度来调整枪榴弹的射程。[2]\xa0'
            '枪榴弹发射具装在枪口使用，多数现代步枪的枪管口部可兼作枪榴弹发射具。用于发射枪榴弹的枪弹有普通枪弹与空包弹，用普通枪弹发射的枪榴弹必须有弹头吸收器、过弹器或弹头偏离装置。常用的是安装在枪榴弹底部尾管内的弹头吸收器。发射时，弹头吸收器将射来的弹头阻止在弹底部，并将弹头的能量和火药燃气压力转换成枪榴弹动能，使枪榴弹获得一定的初速。有的弹头吸收器还装有辅助装药，可使枪榴弹的飞行速度进一步提高。有过弹器的枪榴弹在战斗部的中心有一条允许弹头经过的通道，有弹头偏离装置的枪榴弹在弹底装有一个使弹头从侧面飞离的装置和气体偏流器。这两种枪榴弹都没有弹头吸收器，但都可以用普通枪弹发射。空包弹是一种不装弹头的枪弹，发射时靠火药燃气推动枪榴弹运动。用空包弹发射的枪榴弹一般重量较大、射程较近。但具有结构比较简单和安全性较高等优点。杀伤枪榴弹的直径一般为35～65毫米，弹全重200～600克，射程300～700米，杀伤半径10～30米。破甲枪榴弹的直径为40～75毫米，弹重300～1000克，直射距离50～100米，垂直穿透装甲钢板的厚度为100～300毫米，穿透混凝土的厚度为300～1000毫米。其他枪榴弹的尺寸根据其战术使命确定。[2]\xa0'
            '枪榴弹出现较早，19世纪中叶出现了杀伤枪榴弹。第二次世界大战前夕，英国、美国、日本和德国等国家研制成采用尾翼稳定或旋转稳定，直接插入枪口或用杯形发射器发射的杀伤枪榴弹和破甲枪榴弹。20世纪50年代以后，许多国家将制式步枪的枪管口部设计成枪榴弹发射具，发明了弹头吸收器、过弹器和弹头偏离装置等，可用普通枪弹发射枪榴弹。随后，火箭增程枪榴弹、伸缩式枪榴弹、跳炸枪榴弹、多用途枪榴弹等相继问世。法国、比利时和以色列等国还研制成功由杀伤、破甲、燃烧等枪榴弹组成的枪榴弹系列。枪榴弹的发展趋势是：应用新技术，实现弹重轻量化、弹种系列化、功能多元化；探索和采用新工艺、新结构和改进发射方式，如采用增程火箭，增大有效射程等。随着科学技术的发展，新一代枪榴弹的综合性能将大幅度提高。[2]\xa0'
            '当前枪榴弹的主要发展趋势是：1.弹质量减小，弹种系列化枪榴弹减小弹质量、增大威力是枪榴弹改进设计的目的。关键在于战斗部结构的改进和新材料的应用，通过合理调配弹质量、威力与后坐力三者之间的关系，使枪榴弹达到最佳作战效果。另外，为了满足步兵在未来战场上对付各种目标的需要，枪榴弹正在向着弹种系列化的方向发展。弹种系列化不仅为步兵完成各种不同战斗任务创造了条件，而且给生产、使用、贮存、后勤供应带来极大方便。2.采用实弹发射，简化操作程序枪榴弹采用空包弹发射不仅操作程序复杂，容易贻误战机，而且一旦误装实弹必将造成严重后果。因此，通过捕弹器采用实弹发射是枪榴弹发展的必由之路。捕弹器除了朝通用化、绝对安全型方向发展外，还发展了弹丸偏转器与其相结合的结构。3.改进发射方式，增大有效射程枪榴弹采用实弹发射固然有许多优点，但由于各种枪榴弹的发射装药是一定的，当枪榴弹弹形和弹质量确定后，其射程即为定值。因此，如何改进枪榴弹发射方式，提高枪榴弹的有效射程和射击精度是枪榴弹发展的又一重大课题。利用火箭增程虽可提高枪榴弹射程（可达800m），但仍需解决后坐力大、弹质量大和精度差的问题。4.应用新技术，探索新结构枪榴弹由于受到体积和质量的限制，故新材料、新原理、新结构、新技术在枪榴弹上的应用显得格外重要。当前尽管高强度、高韧性的非金属材料已在枪榴弹上普遍采用，压电引信、电磁引信与其他电子引信正在逐步配置，钢丝缠绕预制破片弹体、高能炸药、新的破甲战斗部结构等已在枪榴弹上应用，但一些传统的概念和结构仍未打破。为全面改善枪榴弹结构，大幅度提高其性能，还有许多技术问题，特别是在发射原理和战斗效能上，需要进一步探索和解决。展望未来，作为步兵简易武器的枪榴弹，在技术上将有新的发展。[1]\xa0',
 'date': '',
 'keyword': '枪榴弹',
 'source': 'baidu',
 'title': '枪榴弹',
 'url': 'https://baike.baidu.com/item/%E6%9E%AA%E6%A6%B4%E5%BC%B9'}
2021-10-26 13:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/baidu?word=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on> (referer: None)
2021-10-26 13:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/baidu?word=%E6%9E%AA%E6%A6%B4%E5%BC%B9&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on> (referer: None)
2021-10-26 13:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E6%9E%AA%E6%A6%B4%E5%BC%B9&ns0=1>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://nasasearch.nasa.gov/search?query=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&affiliate=nasa&utf8=%E2%9C%93>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET http://arc.aiaa.org/action/doSearch?AllField=%E6%9E%AA%E6%A6%B4%E5%BC%B9&sortBy=Earliest&startPage=0&pageSize=20&>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET http://arc.aiaa.org/action/doSearch?AllField=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&sortBy=Earliest&startPage=0&pageSize=20&>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2021-10-26 13:41:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://en.wikipedia.org/w/index.php?title=Special:Search&limit=20&offset=0&profile=default&search=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&ns0=1>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\middleware.py", line 42, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:41:11 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:41:11 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1BEE448>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:41:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 401,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 48828,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 85.628016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 41, 11, 592241),
 'item_scraped_count': 1,
 'log_count/CRITICAL': 16,
 'log_count/DEBUG': 58,
 'log_count/ERROR': 11,
 'log_count/INFO': 101,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 45, 964225)}
2021-10-26 13:41:11 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:41:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://baike.baidu.com/item/%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0/5794> (referer: None)
2021-10-26 13:41:11 [TextProcessorScrapy.spiders.Tiexue] INFO: Tiexue Spider Starting!
2021-10-26 13:41:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/baidu?word=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&tn=bds&cl=3&ct=2097152&si=tiexue.net&s=on> (referer: None)
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "E:\workspace\TextProcessor\TextProcessorScrapy\spiders\Tiexue.py", line 63, in parse
    yield scrapy.Request(url=href, callback=self.new_parse)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\http\request\__init__.py", line 68, in _set_url
    raise ValueError('Missing scheme in request url: %s' % self._url)
ValueError: Missing scheme in request url: /sf/vsearch?pd=video&wd=%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0&tn=vsearch&lid=920cdc34000120f2&ie=utf-8&rsv_pq=920cdc34000120f2&rsv_spt=5&rsv_bp=1&f=8
2021-10-26 13:41:38 [TextProcessorScrapy.spiders.Tiexue] INFO: Tiexue Spider Starting!
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:00 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE19617C8>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1424,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 195.246182,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 0, 149221),
 'log_count/CRITICAL': 20,
 'log_count/DEBUG': 59,
 'log_count/ERROR': 13,
 'log_count/INFO': 116,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 44, 903039)}
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:00 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1C2ADC8>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1361,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 194.157771,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 0, 151634),
 'log_count/CRITICAL': 12,
 'log_count/DEBUG': 52,
 'log_count/ERROR': 14,
 'log_count/INFO': 89,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 45, 993863)}
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:00 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1C7E308>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1469,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 194.129268,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 0, 152630),
 'log_count/CRITICAL': 8,
 'log_count/DEBUG': 52,
 'log_count/ERROR': 15,
 'log_count/INFO': 72,
 'log_count/WARNING': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 46, 23362)}
2021-10-26 13:43:00 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:00 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:2601/session/0aa9e77ba0ce873c39ca3851271c939a/url {"url": "https://baike.baidu.com/item/%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0/5794"}
2021-10-26 13:43:00 [urllib3.connectionpool] DEBUG: Resetting dropped connection: 127.0.0.1
2021-10-26 13:43:02 [urllib3.util.retry] DEBUG: Incremented Retry for (url='/session/0aa9e77ba0ce873c39ca3851271c939a/url'): Retry(total=2, connect=None, read=None, redirect=None, status=None)
2021-10-26 13:43:02 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020EE1CA2FC8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')': /session/0aa9e77ba0ce873c39ca3851271c939a/url
2021-10-26 13:43:02 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (2): 127.0.0.1
2021-10-26 13:43:04 [urllib3.util.retry] DEBUG: Incremented Retry for (url='/session/0aa9e77ba0ce873c39ca3851271c939a/url'): Retry(total=1, connect=None, read=None, redirect=None, status=None)
2021-10-26 13:43:04 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020EE2E0C488>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')': /session/0aa9e77ba0ce873c39ca3851271c939a/url
2021-10-26 13:43:04 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (3): 127.0.0.1
2021-10-26 13:43:06 [urllib3.util.retry] DEBUG: Incremented Retry for (url='/session/0aa9e77ba0ce873c39ca3851271c939a/url'): Retry(total=0, connect=None, read=None, redirect=None, status=None)
2021-10-26 13:43:06 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020EE2E0C988>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')': /session/0aa9e77ba0ce873c39ca3851271c939a/url
2021-10-26 13:43:06 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (4): 127.0.0.1
2021-10-26 13:43:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://baike.baidu.com/item/%E8%88%AA%E7%A9%BA%E6%AF%8D%E8%88%B0/5794> (referer: None)
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "D:\Anaconda\envs\HS\lib\http\client.py", line 1277, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "D:\Anaconda\envs\HS\lib\http\client.py", line 1323, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "D:\Anaconda\envs\HS\lib\http\client.py", line 1272, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "D:\Anaconda\envs\HS\lib\http\client.py", line 1032, in _send_output
    self.send(msg)
  File "D:\Anaconda\envs\HS\lib\http\client.py", line 972, in send
    self.connect()
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000020EE2E10AC8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 117, in iter_errback
    yield next(it)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\python.py", line 345, in __next__
    return next(self.data)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\referer.py", line 338, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\spidermw.py", line 64, in _evaluate_iterable
    for r in iterable:
  File "E:\workspace\TextProcessor\TextProcessorScrapy\spiders\Baidu.py", line 85, in parse
    self.browser.get(response.url)
  File "D:\Anaconda\envs\HS\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "D:\Anaconda\envs\HS\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 319, in execute
    response = self.command_executor.execute(driver_command, params)
  File "D:\Anaconda\envs\HS\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 374, in execute
    return self._request(command_info[0], url, body=data)
  File "D:\Anaconda\envs\HS\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 397, in _request
    resp = self._conn.request(method, url, body=body, headers=headers)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\request.py", line 70, in request
    **urlopen_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\request.py", line 148, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\poolmanager.py", line 321, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 668, in urlopen
    **response_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 668, in urlopen
    **response_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 668, in urlopen
    **response_kw)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=2601): Max retries exceeded with url: /session/0aa9e77ba0ce873c39ca3851271c939a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020EE2E10AC8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:08 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1C5AF88>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1345,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 202.413279,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 8, 422662),
 'log_count/CRITICAL': 10,
 'log_count/DEBUG': 60,
 'log_count/ERROR': 17,
 'log_count/INFO': 85,
 'log_count/WARNING': 4,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 46, 9383)}
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:08 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1CEF708>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 1386,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 201.340293,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 8, 424657),
 'log_count/CRITICAL': 2,
 'log_count/DEBUG': 53,
 'log_count/ERROR': 18,
 'log_count/INFO': 48,
 'log_count/WARNING': 4,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 47, 84364)}
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:43:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:43:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-26 13:43:08 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TextCrawlDownloaderMiddleware.spider_closed of <TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware object at 0x0000020EE1CA4188>>
Traceback (most recent call last):
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\defer.py", line 161, in maybeDeferred_coro
    result = f(*args, **kw)
  File "D:\Anaconda\envs\HS\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 119, in spider_closed
    self.collect(spider)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 130, in collect
    @app.route('/{}'.format(str(COUNT)))
NameError: name 'app' is not defined
2021-10-26 13:43:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 935,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 475230,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 201.390266,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 26, 5, 43, 8, 443470),
 'log_count/CRITICAL': 6,
 'log_count/DEBUG': 60,
 'log_count/ERROR': 19,
 'log_count/INFO': 74,
 'log_count/WARNING': 4,
 'response_received_count': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/MaxRetryError': 1,
 'start_time': datetime.datetime(2021, 10, 26, 5, 39, 47, 53204)}
2021-10-26 13:43:08 [scrapy.core.engine] INFO: Spider closed (finished)
2021-10-26 13:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.baidu.com/link?url=l0PxNu734YfaWaofXotKsbnNMiBt7WTgc3BrTPEZiO5wQCApuUVax-t8aprSMMwi38MZmS1Nsv1g51fjpSsK4_> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.baidu.com/link?url=t78JGtWAbKGOVf04EXevO7ya8hGZp9OnByRRF0mgML-6gyiFb8V0XB5ftrkUJc-XAPI-H71jHlmGVcXiDODRR_> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.baidu.com/link?url=gO7UsA_Q5oAGHPW8WiI5Dq0f6yLPrN-SvecMv2AXT1-LCNs8ngipNZ5HKkrwMLVNsNGLN88xNq4fi0ItJFprxK> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2021-10-26 13:43:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.baidu.com/link?url=8b8wkhLm0qdQ4je1JQrz9Hsj3ayQU_KPTry8MjVLenPkEsTnqIycdCo99iofq81Rddj5VfElpc9BOAYCkGETDK> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]

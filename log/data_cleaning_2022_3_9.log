[2022-03-09 11:06:32] [    INFO] [  __main__ ] - TextCrawler On! (MultisiteSchedule.py:373)
[2022-03-09 11:06:32] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:32] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:33] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43367/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:33] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:43367 (connectionpool.py:232)
[2022-03-09 11:06:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43367 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43367/session/7a7e986dcf5e62bfef361c628a53c6a6/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43367 "POST /session/7a7e986dcf5e62bfef361c628a53c6a6/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:34] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:34] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:34] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:34] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:34] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:34] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:34] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:57399/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:35] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:57399 (connectionpool.py:232)
[2022-03-09 11:06:35] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:57399 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:57399/session/bac7874b957263f58bb58cd5e364852c/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:35] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:57399 "POST /session/bac7874b957263f58bb58cd5e364852c/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:35] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:35] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:35] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:35] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:35] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:35] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:35] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:35] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49555/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:36] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:49555 (connectionpool.py:232)
[2022-03-09 11:06:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49555 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49555/session/1301092382383122fe01bab05bc3f8e0/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49555 "POST /session/1301092382383122fe01bab05bc3f8e0/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:36] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:36] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:36] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:36] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:36] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:36] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:36] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38787/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:37] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38787 (connectionpool.py:232)
[2022-03-09 11:06:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38787 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38787/session/496a74aa4e0574cd739711fc50ad111e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38787 "POST /session/496a74aa4e0574cd739711fc50ad111e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:37] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:37] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:37] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:37] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:37] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:37] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:37] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:37] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:38] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:56299/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:38] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:56299 (connectionpool.py:232)
[2022-03-09 11:06:39] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:56299 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:56299/session/70b786b5a0659393395730ecf018cf83/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:39] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:56299 "POST /session/70b786b5a0659393395730ecf018cf83/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:39] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:39] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:39] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:39] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:39] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:39] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:39] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:39] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51531/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:40] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51531 (connectionpool.py:232)
[2022-03-09 11:06:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51531 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51531/session/b8c4a94501da0dbf450bc33251dffaf7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51531 "POST /session/b8c4a94501da0dbf450bc33251dffaf7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:40] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:40] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:40] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:40] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:40] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:40] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:40] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50669/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:41] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:50669 (connectionpool.py:232)
[2022-03-09 11:06:41] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50669 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50669/session/3a1ed838f27c7f66102dd475a6ff5e1f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:41] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50669 "POST /session/3a1ed838f27c7f66102dd475a6ff5e1f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:41] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:41] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:41] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:41] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:41] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:41] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:41] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:41] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35509/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:42] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:35509 (connectionpool.py:232)
[2022-03-09 11:06:42] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35509 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35509/session/4ccd0fa50afc020f597012b1d550beff/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:42] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35509 "POST /session/4ccd0fa50afc020f597012b1d550beff/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:42] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:42] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:42] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:42] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:42] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:42] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:42] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:42] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41615/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:43] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:41615 (connectionpool.py:232)
[2022-03-09 11:06:43] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41615 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41615/session/871fc478bb7e85551f5a7fe0f92990bf/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:43] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41615 "POST /session/871fc478bb7e85551f5a7fe0f92990bf/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:43] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:43] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:43] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:43] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:43] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:43] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:43] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60141/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:44] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:60141 (connectionpool.py:232)
[2022-03-09 11:06:44] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60141 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60141/session/2d3868f7b1dedb8e466bb85579f92fd2/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:44] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60141 "POST /session/2d3868f7b1dedb8e466bb85579f92fd2/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:44] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:44] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:44] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:44] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:44] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:44] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:44] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:45] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58345/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:45] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58345 (connectionpool.py:232)
[2022-03-09 11:06:45] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58345 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:45] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:45] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58345/session/e5ffcdc2799d8082e7fd7844401e7df7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:45] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58345 "POST /session/e5ffcdc2799d8082e7fd7844401e7df7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:45] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:45] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:45] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:45] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:45] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:45] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:45] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:45] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:45] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38745/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:46] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38745 (connectionpool.py:232)
[2022-03-09 11:06:46] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38745 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38745/session/5eacd19a55b8256d703382b7a9f10fa2/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:46] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38745 "POST /session/5eacd19a55b8256d703382b7a9f10fa2/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:46] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:46] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:46] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:46] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:46] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:46] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:46] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:46] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37237/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:47] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37237 (connectionpool.py:232)
[2022-03-09 11:06:47] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37237 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37237/session/31b6038241899df65c613e11dba12f4f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:47] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37237 "POST /session/31b6038241899df65c613e11dba12f4f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:47] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:47] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:47] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:47] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:47] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:47] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:47] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:47] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:48] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:57097/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:48] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:57097 (connectionpool.py:232)
[2022-03-09 11:06:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:57097 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:57097/session/57adb9420ecd0923ccc9ec8c2291b85d/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:57097 "POST /session/57adb9420ecd0923ccc9ec8c2291b85d/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:49] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:49] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:49] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:49] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:49] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:49] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:49] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:50] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36511/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:50] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36511 (connectionpool.py:232)
[2022-03-09 11:06:50] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36511 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:50] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:50] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36511/session/0b7bb5377d76e5561db47195b9c8004e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:50] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36511 "POST /session/0b7bb5377d76e5561db47195b9c8004e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:50] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:50] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:50] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:50] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:50] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:50] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:50] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:50] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:50] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:51] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40919/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:51] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40919 (connectionpool.py:232)
[2022-03-09 11:06:51] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40919 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:51] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:51] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40919/session/cdfbd244acf02245e72b3770400e4bd0/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:51] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40919 "POST /session/cdfbd244acf02245e72b3770400e4bd0/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:51] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:51] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:51] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:51] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:51] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:51] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:51] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:51] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:51] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:48861/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:52] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:48861 (connectionpool.py:232)
[2022-03-09 11:06:52] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:48861 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:48861/session/8cdc675191943efb5ffdcf8f2e17895e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:52] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:48861 "POST /session/8cdc675191943efb5ffdcf8f2e17895e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:52] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:52] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:52] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:52] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:52] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:52] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:52] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38287/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:53] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38287 (connectionpool.py:232)
[2022-03-09 11:06:53] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38287 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38287/session/d26c3b06bbe0f59d2bf15d092ad9f067/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:53] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38287 "POST /session/d26c3b06bbe0f59d2bf15d092ad9f067/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:53] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:53] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:53] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:53] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:53] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:53] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:53] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58421/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:54] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58421 (connectionpool.py:232)
[2022-03-09 11:06:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58421 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58421/session/248b628676e87e380cdd8dc596a1481a/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58421 "POST /session/248b628676e87e380cdd8dc596a1481a/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:54] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:54] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:54] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:54] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:54] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:54] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:54] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:54] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44301/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:55] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:44301 (connectionpool.py:232)
[2022-03-09 11:06:55] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44301 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44301/session/7f92e45b0a60b6543c20a69b4d47fa20/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:55] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44301 "POST /session/7f92e45b0a60b6543c20a69b4d47fa20/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:55] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:55] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:55] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:55] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:55] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:55] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:55] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:55] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46029/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:56] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:46029 (connectionpool.py:232)
[2022-03-09 11:06:57] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46029 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46029/session/dbfc8a0ec242f38c65665cddbd0f38aa/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:57] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46029 "POST /session/dbfc8a0ec242f38c65665cddbd0f38aa/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:57] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:57] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:57] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:57] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:57] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:57] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:57] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:57] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41855/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:58] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:41855 (connectionpool.py:232)
[2022-03-09 11:06:58] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41855 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41855/session/ac0ce2a26be652b8994605441ec6113f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:58] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41855 "POST /session/ac0ce2a26be652b8994605441ec6113f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:58] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:58] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:58] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:58] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:58] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:58] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:58] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:58] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:06:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51049/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:06:59] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51049 (connectionpool.py:232)
[2022-03-09 11:06:59] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51049 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:06:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51049/session/4c2705e368b97ef90095e138595faa40/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:06:59] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51049 "POST /session/4c2705e368b97ef90095e138595faa40/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:06:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:06:59] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:06:59] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:06:59] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:06:59] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:06:59] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:06:59] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:06:59] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:06:59] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51015/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:00] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51015 (connectionpool.py:232)
[2022-03-09 11:07:00] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51015 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51015/session/7af0b9fce5d53064bbcaf1e56fdaee3d/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:00] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51015 "POST /session/7af0b9fce5d53064bbcaf1e56fdaee3d/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:00] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:00] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:00] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:00] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:00] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:00] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:00] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:01] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38309/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:01] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38309 (connectionpool.py:232)
[2022-03-09 11:07:01] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38309 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:01] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:01] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38309/session/06d3f9fb6c7206be2df1b0d5eaa57c6e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:01] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38309 "POST /session/06d3f9fb6c7206be2df1b0d5eaa57c6e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:01] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:01] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:01] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:01] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:01] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:01] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:01] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:01] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:01] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:45657/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:02] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:45657 (connectionpool.py:232)
[2022-03-09 11:07:02] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:45657 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:45657/session/582ab127f4bc01daa39b9cc8c9892c22/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:02] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:45657 "POST /session/582ab127f4bc01daa39b9cc8c9892c22/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:02] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:02] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:02] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:02] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:02] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:02] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:02] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:02] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40727/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:03] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40727 (connectionpool.py:232)
[2022-03-09 11:07:03] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40727 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40727/session/dc6b3bc54cb76eceeefb9973f2c4d754/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:03] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40727 "POST /session/dc6b3bc54cb76eceeefb9973f2c4d754/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:03] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:03] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:03] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:03] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:03] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:03] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:03] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44987/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:04] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:44987 (connectionpool.py:232)
[2022-03-09 11:07:05] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44987 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:05] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:05] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44987/session/bfd10a44e21e54033a2c9a480985e859/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:05] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44987 "POST /session/bfd10a44e21e54033a2c9a480985e859/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:05] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:05] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:05] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:05] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:05] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:05] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:05] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:05] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:05] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:34913/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:06] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:34913 (connectionpool.py:232)
[2022-03-09 11:07:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:34913 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:34913/session/296cbf4da8569f9c8dbb4a3c66479337/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:34913 "POST /session/296cbf4da8569f9c8dbb4a3c66479337/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:06] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:06] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:06] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:06] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:06] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:06] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:06] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36509/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:07] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36509 (connectionpool.py:232)
[2022-03-09 11:07:07] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36509 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36509/session/524f500f9552b838651a24beeb8a6cb1/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:07] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36509 "POST /session/524f500f9552b838651a24beeb8a6cb1/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:07] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:07] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:07] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:07] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:07] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:07] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:07] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:07] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47439/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:08] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:47439 (connectionpool.py:232)
[2022-03-09 11:07:09] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47439 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47439/session/2be15c5bc4193d4c6dc3de493886910c/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:09] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47439 "POST /session/2be15c5bc4193d4c6dc3de493886910c/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:09] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:09] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:09] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:09] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:09] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:09] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:09] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:09] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39613/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:10] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39613 (connectionpool.py:232)
[2022-03-09 11:07:10] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39613 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39613/session/125c8a9d65cc7f1f50b29cf4e921b170/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:10] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39613 "POST /session/125c8a9d65cc7f1f50b29cf4e921b170/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:10] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:10] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:10] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:10] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:10] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:10] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:10] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:10] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36181/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:11] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36181 (connectionpool.py:232)
[2022-03-09 11:07:11] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36181 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36181/session/00fb3b1d23ca7009ac44497456886fe9/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:11] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36181 "POST /session/00fb3b1d23ca7009ac44497456886fe9/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:11] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:11] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:11] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:11] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:11] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:11] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:11] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:11] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43851/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:12] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:43851 (connectionpool.py:232)
[2022-03-09 11:07:12] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43851 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43851/session/897e76f9ee43949451d8b17824d84929/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:12] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43851 "POST /session/897e76f9ee43949451d8b17824d84929/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:12] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:12] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:12] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:12] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:12] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:12] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:12] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:12] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:13] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55439/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:13] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55439 (connectionpool.py:232)
[2022-03-09 11:07:14] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55439 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55439/session/d18111a81a18091f7e72b574e53461b1/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:14] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55439 "POST /session/d18111a81a18091f7e72b574e53461b1/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:14] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:14] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:14] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:14] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:14] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:14] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:14] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:14] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39929/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:15] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39929 (connectionpool.py:232)
[2022-03-09 11:07:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39929 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39929/session/bbbe27484289e1900b0660872ed9e384/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39929 "POST /session/bbbe27484289e1900b0660872ed9e384/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:15] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:15] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:15] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:15] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:15] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:15] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:15] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:15] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:16] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55859/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:16] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55859 (connectionpool.py:232)
[2022-03-09 11:07:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55859 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55859/session/12d0ccbc11980cd4d115f3b538150470/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55859 "POST /session/12d0ccbc11980cd4d115f3b538150470/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:18] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:18] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:18] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:18] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:18] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:18] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:18] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:18] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41963/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:19] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:41963 (connectionpool.py:232)
[2022-03-09 11:07:19] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41963 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41963/session/6b3bf0d5cd85f50bb16cd8787730654e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:19] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41963 "POST /session/6b3bf0d5cd85f50bb16cd8787730654e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:19] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:19] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:19] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:19] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:19] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:19] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:19] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:19] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59365/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:20] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:59365 (connectionpool.py:232)
[2022-03-09 11:07:20] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59365 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59365/session/4253eefe3df03f91679078cd802f2036/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:20] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59365 "POST /session/4253eefe3df03f91679078cd802f2036/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:20] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:20] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:20] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:20] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:20] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:20] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:20] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:21] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:22] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53555/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:22] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:53555 (connectionpool.py:232)
[2022-03-09 11:07:23] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53555 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53555/session/2fa4bf7a434216dd6e2d0b60293e338a/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:23] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53555 "POST /session/2fa4bf7a434216dd6e2d0b60293e338a/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:23] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:23] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:23] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:23] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:23] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:23] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:23] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:23] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43353/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:24] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:43353 (connectionpool.py:232)
[2022-03-09 11:07:25] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43353 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:25] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:25] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43353/session/48479ad812d50c8cfa3d237ed61494a7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:25] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43353 "POST /session/48479ad812d50c8cfa3d237ed61494a7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:25] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:25] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:25] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:25] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:25] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:25] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:25] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:25] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:25] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58749/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:26] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58749 (connectionpool.py:232)
[2022-03-09 11:07:26] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58749 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58749/session/5eb789b0c1735e5283ecd2a6e5d48587/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:26] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58749 "POST /session/5eb789b0c1735e5283ecd2a6e5d48587/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:26] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:26] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:26] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:26] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:26] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:26] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:26] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:26] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47957/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:27] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:47957 (connectionpool.py:232)
[2022-03-09 11:07:28] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47957 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47957/session/3c60c96130779192021a414b18195c3d/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:28] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47957 "POST /session/3c60c96130779192021a414b18195c3d/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:28] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:28] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:28] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:28] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:28] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:28] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:28] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33079/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:29] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:33079 (connectionpool.py:232)
[2022-03-09 11:07:30] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33079 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:30] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:30] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33079/session/c704fb1763226887bd6ce7a6e260f63e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:30] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33079 "POST /session/c704fb1763226887bd6ce7a6e260f63e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:30] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:30] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:30] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:30] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:30] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:30] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:30] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:30] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:30] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37277/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:31] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37277 (connectionpool.py:232)
[2022-03-09 11:07:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37277 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37277/session/57be562837d95b052fa40a436fef762b/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37277 "POST /session/57be562837d95b052fa40a436fef762b/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:34] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:34] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:34] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:34] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:34] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:34] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:34] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38265/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:35] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38265 (connectionpool.py:232)
[2022-03-09 11:07:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38265 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38265/session/31ca931e9df1167b516f1ace8df55425/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38265 "POST /session/31ca931e9df1167b516f1ace8df55425/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:36] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:36] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:36] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:36] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:36] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:36] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:36] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37815/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:37] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37815 (connectionpool.py:232)
[2022-03-09 11:07:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37815 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37815/session/afcd708c8a5fae989a0da0caa53fe1f4/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37815 "POST /session/afcd708c8a5fae989a0da0caa53fe1f4/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:37] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:37] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:37] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:37] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:37] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:37] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:37] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:37] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:38] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38639/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:38] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38639 (connectionpool.py:232)
[2022-03-09 11:07:39] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38639 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38639/session/0077c103f9093286e667761ac3f671ff/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:39] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38639 "POST /session/0077c103f9093286e667761ac3f671ff/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:39] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:39] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:39] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:39] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:39] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:39] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:39] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:39] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58833/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:40] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58833 (connectionpool.py:232)
[2022-03-09 11:07:42] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58833 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:07:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58833/session/15d9c4a0e8840871975bc9dabe6549ae/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:07:42] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58833 "POST /session/15d9c4a0e8840871975bc9dabe6549ae/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:07:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:07:42] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:07:42] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:07:42] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:07:42] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:07:42] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:07:42] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:07:42] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:07:42] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:07:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50487/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:07:43] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:50487 (connectionpool.py:232)
[2022-03-09 11:08:35] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50487 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:08:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50487/session/5103ed31ed822239fa8148791b799d3f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:08:35] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50487 "POST /session/5103ed31ed822239fa8148791b799d3f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:08:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:35] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:08:35] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:08:35] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:08:35] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:08:35] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:08:35] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:08:35] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:08:35] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:08:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43919/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:08:36] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:43919 (connectionpool.py:232)
[2022-03-09 11:08:44] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43919 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:08:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43919/session/f594379dd53ab0f1a16e2b388972e335/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:08:44] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43919 "POST /session/f594379dd53ab0f1a16e2b388972e335/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:08:44] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:44] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:08:44] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:08:44] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:08:44] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:08:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:08:44] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:08:44] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:08:44] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:08:45] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40211/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:08:45] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40211 (connectionpool.py:232)
[2022-03-09 11:08:53] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40211 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:08:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40211/session/0151af92b16f0308156fa5d6ba8810fe/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:08:53] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40211 "POST /session/0151af92b16f0308156fa5d6ba8810fe/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:08:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:08:53] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:08:53] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:08:53] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:08:53] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:08:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:08:53] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:08:53] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:08:53] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:08:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51525/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:08:55] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51525 (connectionpool.py:232)
[2022-03-09 11:09:03] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51525 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51525/session/8c55cc2cb08586922f4f553e9680375e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:03] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51525 "POST /session/8c55cc2cb08586922f4f553e9680375e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:03] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:03] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:03] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:03] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:03] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:03] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:03] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40899/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:04] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40899 (connectionpool.py:232)
[2022-03-09 11:09:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40899 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40899/session/d2a0db72a5d3f394fc506bb4d75f69b7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40899 "POST /session/d2a0db72a5d3f394fc506bb4d75f69b7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:06] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:06] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:06] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:06] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:06] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:06] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:06] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40103/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:07] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40103 (connectionpool.py:232)
[2022-03-09 11:09:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40103 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40103/session/1c761bb7868cc1d5266a6e002fbb251c/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40103 "POST /session/1c761bb7868cc1d5266a6e002fbb251c/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:08] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:08] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:08] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:08] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:08] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:08] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:08] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33945/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:09] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:33945 (connectionpool.py:232)
[2022-03-09 11:09:21] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33945 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33945/session/f23ed1f90c70aacb6c1693412502af9e/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:21] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33945 "POST /session/f23ed1f90c70aacb6c1693412502af9e/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:21] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:21] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:21] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:21] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:21] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:21] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:21] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:21] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36089/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:24] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36089 (connectionpool.py:232)
[2022-03-09 11:09:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36089 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36089/session/2db26100feb62e69b595cc5576df5ffe/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36089 "POST /session/2db26100feb62e69b595cc5576df5ffe/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:40] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:40] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:40] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:40] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:40] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:41] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:41] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:42] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49805/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:42] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:49805 (connectionpool.py:232)
[2022-03-09 11:09:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49805 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49805/session/801659b4774aeddcbbb5b0f9deee1e6f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49805 "POST /session/801659b4774aeddcbbb5b0f9deee1e6f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:49] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:49] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:49] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:49] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:49] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:49] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:49] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:50] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35679/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:50] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:35679 (connectionpool.py:232)
[2022-03-09 11:09:52] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35679 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35679/session/1a4b712be3b19c9c3e22fbe1f6d2d120/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:52] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35679 "POST /session/1a4b712be3b19c9c3e22fbe1f6d2d120/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:52] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:52] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:52] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:52] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:52] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:52] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:52] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:52] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53931/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:53] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:53931 (connectionpool.py:232)
[2022-03-09 11:09:56] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53931 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:09:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53931/session/31b8a3356bcd6801238eb6a8b7be73a6/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:09:56] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53931 "POST /session/31b8a3356bcd6801238eb6a8b7be73a6/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:09:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:09:56] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:09:56] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:09:56] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:09:56] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:09:56] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:09:56] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:09:56] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:09:56] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:09:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40591/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:09:58] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40591 (connectionpool.py:232)
[2022-03-09 11:10:00] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40591 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:10:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40591/session/14faee61da315931f8a750be54d32eb7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:10:00] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40591 "POST /session/14faee61da315931f8a750be54d32eb7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:10:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:00] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:10:00] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:10:00] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:10:00] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:10:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:00] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:10:00] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:10:00] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:10:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44019/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:10:02] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:44019 (connectionpool.py:232)
[2022-03-09 11:10:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44019 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:10:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44019/session/922cf749a625f32d98503a03d689c856/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:10:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44019 "POST /session/922cf749a625f32d98503a03d689c856/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:10:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:08] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:10:08] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:10:08] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:10:08] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:10:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:08] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:10:08] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:10:08] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:10:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:48959/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:10:10] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:48959 (connectionpool.py:232)
[2022-03-09 11:10:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:48959 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:10:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:48959/session/f9bb0338fbd6d970334f5ed3fe6ff259/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:10:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:48959 "POST /session/f9bb0338fbd6d970334f5ed3fe6ff259/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:10:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:27] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:10:28] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:10:28] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:10:28] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:10:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:28] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:10:28] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:10:28] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:10:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35317/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:10:29] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:35317 (connectionpool.py:232)
[2022-03-09 11:10:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35317 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:10:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35317/session/9f703dbf9418ddc25e4df58b38ebb822/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:10:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35317 "POST /session/9f703dbf9418ddc25e4df58b38ebb822/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:10:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:36] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:10:36] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:10:36] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:10:36] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:10:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:36] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:10:36] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_0.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:10:36] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:10:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55691/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:10:37] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55691 (connectionpool.py:232)
[2022-03-09 11:10:43] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55691 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:10:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55691/session/33d9db3fe137ae87f8e1680b85794c33/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:10:43] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55691 "POST /session/33d9db3fe137ae87f8e1680b85794c33/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:10:43] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:10:43] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:45] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:46] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:47] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:49] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E6%B3%95%E5%9B%BD%E9%98%B2%E9%95%BF> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:50] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:51] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%8D%E7%BB%93%E7%9B%9F%E8%BF%90%E5%8A%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%A4%A7%E9%80%89%E6%8A%95%E7%A5%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%97%B1%E7%81%BE> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E2%80%9C%E8%AF%AD%E8%A8%80%E5%88%86%E7%A6%BB%E4%B8%BB%E4%B9%89%E2%80%9D%E6%B4%BB%E8%B7%83> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%91%E6%97%8F%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E7%BA%B3%E8%90%A8%E5%B0%94%E5%B7%B4%E9%87%8C%E8%BF%90%E5%8A%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E7%BE%8E%E5%9B%BD%E5%9B%BD%E5%8A%A1%E5%8D%BF> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E9%9D%9E%E6%9A%B4%E5%8A%9B%E4%B8%8D%E5%90%88%E4%BD%9C%E8%BF%90%E5%8A%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AE%AE%E4%BC%9A%E9%80%89%E4%B8%BE> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:54] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:55] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:56] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:57] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:57] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2021%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%9A%B4%E9%9B%A8%E7%81%BE%E5%AE%B3> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:57] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E5%9C%B0%E9%9C%87> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:58] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%86%A0%E7%96%AB%E6%83%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2015%E5%8D%B0%E5%BA%A6%E9%AB%98%E6%B8%A9%E7%81%BE%E5%AE%B3> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B730%E5%8D%B0%E5%BA%A6%E9%A9%AC%E5%93%88%E6%8B%89%E6%96%BD%E7%89%B9%E6%8B%89%E9%82%A6%E6%B3%A5%E7%9F%B3%E6%B5%81> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/11%C2%B712%E5%8D%B0%E5%BA%A6%E5%8D%A1%E7%BA%B3%E5%A1%94%E5%85%8B%E9%82%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:10:59] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:10:59] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B725%E5%8D%B0%E5%BA%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:01] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%A7%E6%B0%94%E5%8D%B1%E6%9C%BA> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B714%E5%8D%B0%E5%BA%A6%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:02] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%97%E9%83%A8%E5%86%B0%E5%B7%9D%E6%96%AD%E8%A3%82> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2019%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%B4%AA%E7%81%BE> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BC%A0%E7%96%AB> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8D%9A%E5%B8%95%E5%B0%94%E7%81%BE%E9%9A%BE> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B717%E5%8D%B0%E5%BA%A6%E5%AF%8C%E5%A3%AB%E5%BA%B7%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6H5N1%E7%A6%BD%E6%B5%81%E6%84%9F> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:04] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%9D%97%E8%99%AB%E8%A2%AD%E5%87%BB%E6%96%B0%E5%BE%B7%E9%87%8C> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:05] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:05] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/5%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%96%E5%B7%A5%E5%8E%82%E6%AF%92%E6%B0%94%E6%B3%84%E6%BC%8F%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:07] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B5%B7%E5%86%9B%E8%88%AA%E9%81%93%E6%BC%94%E4%B9%A0> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:07] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:07] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B7%A1%E8%88%AA%E5%AF%BC%E5%BC%B9%E8%AF%95%E5%B0%84%E5%A4%B1%E8%B4%A5> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:07] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%82%AE%E6%88%98> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:07] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/1962%E5%B9%B4%E6%8C%91%E8%B5%B7%E4%B8%AD%E5%8D%B0%E8%BE%B9%E7%95%8C%E4%BA%89%E7%AB%AF> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:08] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9B%E4%BA%8B%E6%BC%94%E4%B9%A0> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:08] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%86%B2%E7%AA%81> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:08] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E6%B3%95%E5%9B%BD%E9%98%B5%E9%A3%8E%E6%88%98%E6%9C%BA> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:09] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:09] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E7%BE%8E%E5%9B%BDF-16%E6%88%98%E6%9C%BA> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:09] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E6%96%B0%E5%9E%8B%E5%8F%8D%E9%9B%B7%E8%BE%BE%E5%AF%BC%E5%BC%B9> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:10] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:10] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E5%8F%AF%E8%BD%BD%E6%A0%B8%E5%BC%B9%E5%A4%B4%E5%AF%BC%E5%BC%B9> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:11] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:12] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8C%BB%E6%8A%A4%E9%81%87%E8%A2%AD> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:12] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:12] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84K-4%E6%BD%9C%E5%B0%84%E5%AF%BC%E5%BC%B9> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:12] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BB%91%E5%85%AC%E4%BA%A4%E8%BD%AE%E5%A5%B8%E6%A1%88> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B8%A9%E8%B8%8F%E4%BA%8B%E4%BB%B6> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/6%C2%B715%E4%B8%AD%E5%8D%B0%E5%8A%A0%E5%8B%92%E4%B8%87%E6%B2%B3%E8%B0%B7%E8%BE%B9%E5%A2%83%E5%86%B2%E7%AA%81> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%AD%E5%8D%B0%E4%B8%BE%E8%A1%8C%E5%86%9B%E9%95%BF%E7%BA%A7%E4%BC%9A%E6%99%A4> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B0%91%E5%A5%B3%E8%A2%AB%E6%B3%BC%E6%B1%BD%E6%B2%B9%E7%83%A7%E6%AD%BB> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%AD%9F%E4%B9%B0%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:13] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%BC%BA%E5%A5%B8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:14] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:15] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:17] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B72%E5%8D%B0%E5%BA%A6%E6%B0%B4%E5%9D%9D%E6%BA%83%E5%A0%A4%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:17] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/10%C2%B73%E5%8D%B0%E5%BA%A6%E5%B7%B4%E7%89%B9%E9%82%A3%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A%E8%B8%A9%E8%B8%8F%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:17] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E7%A4%BA%E5%A8%81%E6%B8%B8%E8%A1%8C> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:18] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:19] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:20] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B727%E5%8D%B0%E5%BA%A6%E5%AE%A2%E6%9C%BA%E5%86%B2%E5%87%BA%E8%B7%91%E9%81%93%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:20] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:21] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:22] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B712%E5%8D%B0%E5%BA%A6%E5%A4%A7%E5%B7%B4%E4%B8%8E%E5%8D%A1%E8%BD%A6%E7%9B%B8%E6%92%9E%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:22] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/9%C2%B77%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E6%B3%95%E9%99%A2%E7%88%86%E7%82%B8%E6%A1%88> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:22] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/5%C2%B722%E5%8D%B0%E5%BA%A6%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%9D%A0%E6%9C%BA%E4%BA%8B%E4%BB%B6> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:23] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/8%C2%B728%E5%8D%B0%E5%BA%A6%E8%B4%BE%E5%9D%8E%E5%BE%B7%E9%82%A6%E5%A4%A7%E5%9D%9D%E5%9D%8D%E5%A1%8C%E4%BA%8B%E6%95%85> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:23] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B714%E5%8D%B0%E5%BA%A6%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:23] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9C%E6%B0%91%E8%BF%90%E5%8A%A8> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:23] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:25] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:25] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%87%E4%BA%BA%E4%BC%8A%E6%96%AF%E5%85%B0%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:25] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/1%C2%B726%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%97%A5%E6%8A%97%E8%AE%AE%E5%86%B2%E7%AA%81> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:26] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:27] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E5%8C%BB%E7%94%9F%E6%B8%B8%E8%A1%8C> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:27] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/8%C2%B725%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:27] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/9%C2%B72%E5%8D%B0%E5%BA%A6%E5%A4%A7%E7%BD%A2%E5%B7%A5> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:28] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B712%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B715%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B724%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 1 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E6%B3%95%E5%9B%BD%E9%98%B2%E9%95%BF> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%8D%E7%BB%93%E7%9B%9F%E8%BF%90%E5%8A%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:30] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:31] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%A4%A7%E9%80%89%E6%8A%95%E7%A5%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:31] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%97%B1%E7%81%BE> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:32] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E2%80%9C%E8%AF%AD%E8%A8%80%E5%88%86%E7%A6%BB%E4%B8%BB%E4%B9%89%E2%80%9D%E6%B4%BB%E8%B7%83> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:32] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%91%E6%97%8F%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:32] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E7%BA%B3%E8%90%A8%E5%B0%94%E5%B7%B4%E9%87%8C%E8%BF%90%E5%8A%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:33] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E7%BE%8E%E5%9B%BD%E5%9B%BD%E5%8A%A1%E5%8D%BF> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:33] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E9%9D%9E%E6%9A%B4%E5%8A%9B%E4%B8%8D%E5%90%88%E4%BD%9C%E8%BF%90%E5%8A%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:33] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AE%AE%E4%BC%9A%E9%80%89%E4%B8%BE> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:33] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:33] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2021%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%9A%B4%E9%9B%A8%E7%81%BE%E5%AE%B3> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:35] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:35] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:36] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%86%A0%E7%96%AB%E6%83%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:36] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E5%9C%B0%E9%9C%87> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:37] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%A7%E6%B0%94%E5%8D%B1%E6%9C%BA> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:37] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2015%E5%8D%B0%E5%BA%A6%E9%AB%98%E6%B8%A9%E7%81%BE%E5%AE%B3> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:37] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:37] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:38] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B730%E5%8D%B0%E5%BA%A6%E9%A9%AC%E5%93%88%E6%8B%89%E6%96%BD%E7%89%B9%E6%8B%89%E9%82%A6%E6%B3%A5%E7%9F%B3%E6%B5%81> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:38] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/11%C2%B712%E5%8D%B0%E5%BA%A6%E5%8D%A1%E7%BA%B3%E5%A1%94%E5%85%8B%E9%82%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:38] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B725%E5%8D%B0%E5%BA%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:39] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:39] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B714%E5%8D%B0%E5%BA%A6%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:39] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%97%E9%83%A8%E5%86%B0%E5%B7%9D%E6%96%AD%E8%A3%82> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:39] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2019%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%B4%AA%E7%81%BE> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:39] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:41] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:42] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/5%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%96%E5%B7%A5%E5%8E%82%E6%AF%92%E6%B0%94%E6%B3%84%E6%BC%8F%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:42] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:42] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:44] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:45] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:46] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BC%A0%E7%96%AB> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:46] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8D%9A%E5%B8%95%E5%B0%94%E7%81%BE%E9%9A%BE> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:46] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:47] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B717%E5%8D%B0%E5%BA%A6%E5%AF%8C%E5%A3%AB%E5%BA%B7%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:47] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6H5N1%E7%A6%BD%E6%B5%81%E6%84%9F> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:47] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:48] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E8%9D%97%E8%99%AB%E8%A2%AD%E5%87%BB%E6%96%B0%E5%BE%B7%E9%87%8C> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:48] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B5%B7%E5%86%9B%E8%88%AA%E9%81%93%E6%BC%94%E4%B9%A0> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:49] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B7%A1%E8%88%AA%E5%AF%BC%E5%BC%B9%E8%AF%95%E5%B0%84%E5%A4%B1%E8%B4%A5> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:49] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%82%AE%E6%88%98> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:49] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/1962%E5%B9%B4%E6%8C%91%E8%B5%B7%E4%B8%AD%E5%8D%B0%E8%BE%B9%E7%95%8C%E4%BA%89%E7%AB%AF> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:50] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:50] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E6%B3%95%E5%9B%BD%E9%98%B5%E9%A3%8E%E6%88%98%E6%9C%BA> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:51] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:51] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%86%B2%E7%AA%81> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:52] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9B%E4%BA%8B%E6%BC%94%E4%B9%A0> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:52] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:52] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E7%BE%8E%E5%9B%BDF-16%E6%88%98%E6%9C%BA> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:53] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E6%96%B0%E5%9E%8B%E5%8F%8D%E9%9B%B7%E8%BE%BE%E5%AF%BC%E5%BC%B9> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:53] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E5%8F%AF%E8%BD%BD%E6%A0%B8%E5%BC%B9%E5%A4%B4%E5%AF%BC%E5%BC%B9> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:53] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8C%BB%E6%8A%A4%E9%81%87%E8%A2%AD> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84K-4%E6%BD%9C%E5%B0%84%E5%AF%BC%E5%BC%B9> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BB%91%E5%85%AC%E4%BA%A4%E8%BD%AE%E5%A5%B8%E6%A1%88> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:54] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B8%A9%E8%B8%8F%E4%BA%8B%E4%BB%B6> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:54] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:55] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/6%C2%B715%E4%B8%AD%E5%8D%B0%E5%8A%A0%E5%8B%92%E4%B8%87%E6%B2%B3%E8%B0%B7%E8%BE%B9%E5%A2%83%E5%86%B2%E7%AA%81> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:55] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:56] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:57] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:57] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%AD%E5%8D%B0%E4%B8%BE%E8%A1%8C%E5%86%9B%E9%95%BF%E7%BA%A7%E4%BC%9A%E6%99%A4> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:57] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B0%91%E5%A5%B3%E8%A2%AB%E6%B3%BC%E6%B1%BD%E6%B2%B9%E7%83%A7%E6%AD%BB> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%AD%9F%E4%B9%B0%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:58] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%BC%BA%E5%A5%B8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:58] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/7%C2%B72%E5%8D%B0%E5%BA%A6%E6%B0%B4%E5%9D%9D%E6%BA%83%E5%A0%A4%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:59] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:11:59] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/10%C2%B73%E5%8D%B0%E5%BA%A6%E5%B7%B4%E7%89%B9%E9%82%A3%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A%E8%B8%A9%E8%B8%8F%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:59] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E7%A4%BA%E5%A8%81%E6%B8%B8%E8%A1%8C> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:59] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B727%E5%8D%B0%E5%BA%A6%E5%AE%A2%E6%9C%BA%E5%86%B2%E5%87%BA%E8%B7%91%E9%81%93%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:11:59] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B712%E5%8D%B0%E5%BA%A6%E5%A4%A7%E5%B7%B4%E4%B8%8E%E5%8D%A1%E8%BD%A6%E7%9B%B8%E6%92%9E%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:00] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:01] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/9%C2%B77%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E6%B3%95%E9%99%A2%E7%88%86%E7%82%B8%E6%A1%88> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:01] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/5%C2%B722%E5%8D%B0%E5%BA%A6%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%9D%A0%E6%9C%BA%E4%BA%8B%E4%BB%B6> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:02] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:02] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/8%C2%B728%E5%8D%B0%E5%BA%A6%E8%B4%BE%E5%9D%8E%E5%BE%B7%E9%82%A6%E5%A4%A7%E5%9D%9D%E5%9D%8D%E5%A1%8C%E4%BA%8B%E6%95%85> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B714%E5%8D%B0%E5%BA%A6%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:03] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9C%E6%B0%91%E8%BF%90%E5%8A%A8> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:03] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:04] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E4%B8%87%E4%BA%BA%E4%BC%8A%E6%96%AF%E5%85%B0%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:04] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/1%C2%B726%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%97%A5%E6%8A%97%E8%AE%AE%E5%86%B2%E7%AA%81> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:05] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:05] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E5%8C%BB%E7%94%9F%E6%B8%B8%E8%A1%8C> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:05] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/8%C2%B725%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:05] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/9%C2%B72%E5%8D%B0%E5%BA%A6%E5%A4%A7%E7%BD%A2%E5%B7%A5> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:07] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:09] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:10] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:11] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B712%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:11] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:12] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:12] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/12%C2%B715%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:12] [   DEBUG] [scrapy.downloadermiddlewares.retry ] - Retrying <GET https://baike.baidu.com/item/2%C2%B724%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 2 times): DNS lookup failed: http_proxy. (retry.py:99)
[2022-03-09 11:12:13] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E6%B3%95%E5%9B%BD%E9%98%B2%E9%95%BF> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:13] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:13] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E6%B3%95%E5%9B%BD%E9%98%B2%E9%95%BF (Baidu.py:149)
[2022-03-09 11:12:13] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:13] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1338,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 329.056116,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 13, 690007),
 'log_count/DEBUG': 522,
 'log_count/ERROR': 3,
 'log_count/INFO': 606,
 'memusage/max': 76165120,
 'memusage/startup': 68071424,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 44, 633891)} (statscollectors.py:47)
[2022-03-09 11:12:13] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:13] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E4%B8%8D%E7%BB%93%E7%9B%9F%E8%BF%90%E5%8A%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:13] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:13] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E4%B8%8D%E7%BB%93%E7%9B%9F%E8%BF%90%E5%8A%A8 (Baidu.py:149)
[2022-03-09 11:12:13] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:13] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1250,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 337.158601,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 13, 936329),
 'log_count/DEBUG': 571,
 'log_count/ERROR': 6,
 'log_count/INFO': 665,
 'memusage/max': 76165120,
 'memusage/startup': 66682880,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 36, 777728)} (statscollectors.py:47)
[2022-03-09 11:12:13] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:14] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:14] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%A4%A7%E9%80%89%E6%8A%95%E7%A5%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:14] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:14] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%A4%A7%E9%80%89%E6%8A%95%E7%A5%A8 (Baidu.py:149)
[2022-03-09 11:12:14] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:14] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1270,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 333.16489,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 14, 419551),
 'log_count/DEBUG': 543,
 'log_count/ERROR': 9,
 'log_count/INFO': 637,
 'memusage/max': 76165120,
 'memusage/startup': 67485696,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 41, 254661)} (statscollectors.py:47)
[2022-03-09 11:12:14] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:14] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%97%B1%E7%81%BE> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:14] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:14] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%97%B1%E7%81%BE (Baidu.py:149)
[2022-03-09 11:12:14] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:14] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1239,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 329.118209,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 14, 885369),
 'log_count/DEBUG': 515,
 'log_count/ERROR': 12,
 'log_count/INFO': 608,
 'memusage/max': 76165120,
 'memusage/startup': 68272128,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 45, 767160)} (statscollectors.py:47)
[2022-03-09 11:12:14] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:15] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E2%80%9C%E8%AF%AD%E8%A8%80%E5%88%86%E7%A6%BB%E4%B8%BB%E4%B9%89%E2%80%9D%E6%B4%BB%E8%B7%83> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:15] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%91%E6%97%8F%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:15] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E7%BA%B3%E8%90%A8%E5%B0%94%E5%B7%B4%E9%87%8C%E8%BF%90%E5%8A%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E2%80%9C%E8%AF%AD%E8%A8%80%E5%88%86%E7%A6%BB%E4%B8%BB%E4%B9%89%E2%80%9D%E6%B4%BB%E8%B7%83 (Baidu.py:149)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%91%E6%97%8F%E7%8B%AC%E7%AB%8B%E8%BF%90%E5%8A%A8 (Baidu.py:149)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:15] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E7%BA%B3%E8%90%A8%E5%B0%94%E5%B7%B4%E9%87%8C%E8%BF%90%E5%8A%A8 (Baidu.py:149)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:15] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1384,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 336.312498,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 15, 336221),
 'log_count/DEBUG': 557,
 'log_count/ERROR': 21,
 'log_count/INFO': 659,
 'memusage/max': 76165120,
 'memusage/startup': 67084288,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 39, 23723)} (statscollectors.py:47)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:15] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1324,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 340.817837,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 15, 379758),
 'log_count/DEBUG': 585,
 'log_count/ERROR': 21,
 'log_count/INFO': 694,
 'memusage/max': 76165120,
 'memusage/startup': 66187264,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 34, 561921)} (statscollectors.py:47)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:15] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1309,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 335.287578,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 15, 423960),
 'log_count/DEBUG': 550,
 'log_count/ERROR': 21,
 'log_count/INFO': 657,
 'memusage/max': 76165120,
 'memusage/startup': 67297280,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 40, 136382)} (statscollectors.py:47)
[2022-03-09 11:12:15] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:15] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:16] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E7%BE%8E%E5%9B%BD%E5%9B%BD%E5%8A%A1%E5%8D%BF> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:16] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:16] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E8%8E%AB%E8%BF%AA%E4%BC%9A%E8%A7%81%E7%BE%8E%E5%9B%BD%E5%9B%BD%E5%8A%A1%E5%8D%BF (Baidu.py:149)
[2022-03-09 11:12:16] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:16] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1365,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 333.273956,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 16, 774647),
 'log_count/DEBUG': 529,
 'log_count/ERROR': 24,
 'log_count/INFO': 637,
 'memusage/max': 76165120,
 'memusage/startup': 67883008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 43, 500691)} (statscollectors.py:47)
[2022-03-09 11:12:16] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:17] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E9%9D%9E%E6%9A%B4%E5%8A%9B%E4%B8%8D%E5%90%88%E4%BD%9C%E8%BF%90%E5%8A%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E9%9D%9E%E6%9A%B4%E5%8A%9B%E4%B8%8D%E5%90%88%E4%BD%9C%E8%BF%90%E5%8A%A8 (Baidu.py:149)
[2022-03-09 11:12:18] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:18] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1331,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 342.515444,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 18, 185437),
 'log_count/DEBUG': 578,
 'log_count/ERROR': 27,
 'log_count/INFO': 696,
 'memusage/max': 76165120,
 'memusage/startup': 66486272,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 35, 669993)} (statscollectors.py:47)
[2022-03-09 11:12:18] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:18] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AE%AE%E4%BC%9A%E9%80%89%E4%B8%BE> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AE%AE%E4%BC%9A%E9%80%89%E4%B8%BE (Baidu.py:149)
[2022-03-09 11:12:18] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:18] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1277,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 336.033026,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 18, 403105),
 'log_count/DEBUG': 536,
 'log_count/ERROR': 30,
 'log_count/INFO': 651,
 'memusage/max': 76165120,
 'memusage/startup': 67682304,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 42, 370079)} (statscollectors.py:47)
[2022-03-09 11:12:18] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:18] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:18] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:18] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B (Baidu.py:149)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:19] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1304,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 341.104184,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 19, 9819),
 'log_count/DEBUG': 564,
 'log_count/ERROR': 33,
 'log_count/INFO': 687,
 'memusage/max': 76165120,
 'memusage/startup': 66895872,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 37, 905635)} (statscollectors.py:47)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:19] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2021%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%9A%B4%E9%9B%A8%E7%81%BE%E5%AE%B3> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:19] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:19] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2021%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%9A%B4%E9%9B%A8%E7%81%BE%E5%AE%B3 (Baidu.py:149)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:19] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1330,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 327.892891,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 19, 256097),
 'log_count/DEBUG': 480,
 'log_count/ERROR': 36,
 'log_count/INFO': 594,
 'memusage/max': 76165120,
 'memusage/startup': 69173248,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 51, 363206)} (statscollectors.py:47)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:19] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%86%A0%E7%96%AB%E6%83%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:19] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:19] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:19] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%86%A0%E7%96%AB%E6%83%85 (Baidu.py:149)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:19] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1282,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 322.721036,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 19, 753169),
 'log_count/DEBUG': 445,
 'log_count/ERROR': 39,
 'log_count/INFO': 558,
 'memusage/max': 76165120,
 'memusage/startup': 70094848,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 57, 32133)} (statscollectors.py:47)
[2022-03-09 11:12:19] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:20] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E5%9C%B0%E9%9C%87> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E5%9C%B0%E9%9C%87 (Baidu.py:149)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:20] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1306,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 332.235765,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 20, 216073),
 'log_count/DEBUG': 501,
 'log_count/ERROR': 42,
 'log_count/INFO': 625,
 'memusage/max': 76165120,
 'memusage/startup': 68640768,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 47, 980308)} (statscollectors.py:47)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:20] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%A7%E6%B0%94%E5%8D%B1%E6%9C%BA> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:20] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2015%E5%8D%B0%E5%BA%A6%E9%AB%98%E6%B8%A9%E7%81%BE%E5%AE%B3> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:20] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/7%C2%B730%E5%8D%B0%E5%BA%A6%E9%A9%AC%E5%93%88%E6%8B%89%E6%96%BD%E7%89%B9%E6%8B%89%E9%82%A6%E6%B3%A5%E7%9F%B3%E6%B5%81> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B0%A7%E6%B0%94%E5%8D%B1%E6%9C%BA (Baidu.py:149)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2015%E5%8D%B0%E5%BA%A6%E9%AB%98%E6%B8%A9%E7%81%BE%E5%AE%B3 (Baidu.py:149)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:20] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/7%C2%B730%E5%8D%B0%E5%BA%A6%E9%A9%AC%E5%93%88%E6%8B%89%E6%96%BD%E7%89%B9%E6%8B%89%E9%82%A6%E6%B3%A5%E7%9F%B3%E6%B5%81 (Baidu.py:149)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:20] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1289,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 319.032927,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 20, 663583),
 'log_count/DEBUG': 417,
 'log_count/ERROR': 51,
 'log_count/INFO': 532,
 'memusage/max': 76165120,
 'memusage/startup': 70905856,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 1, 630656)} (statscollectors.py:47)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:20] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1282,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 328.183159,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 20, 709245),
 'log_count/DEBUG': 473,
 'log_count/ERROR': 51,
 'log_count/INFO': 599,
 'memusage/max': 76165120,
 'memusage/startup': 69341184,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 52, 526086)} (statscollectors.py:47)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:20] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1471,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 324.834523,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 20, 754903),
 'log_count/DEBUG': 452,
 'log_count/ERROR': 51,
 'log_count/INFO': 578,
 'memusage/max': 76165120,
 'memusage/startup': 69877760,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 55, 920380)} (statscollectors.py:47)
[2022-03-09 11:12:20] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:20] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:21] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:21] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/11%C2%B712%E5%8D%B0%E5%BA%A6%E5%8D%A1%E7%BA%B3%E5%A1%94%E5%85%8B%E9%82%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:21] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:21] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/11%C2%B712%E5%8D%B0%E5%BA%A6%E5%8D%A1%E7%BA%B3%E5%A1%94%E5%85%8B%E9%82%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1 (Baidu.py:149)
[2022-03-09 11:12:22] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:22] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1450,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 327.276419,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 22, 92230),
 'log_count/DEBUG': 459,
 'log_count/ERROR': 54,
 'log_count/INFO': 591,
 'memusage/max': 76165120,
 'memusage/startup': 69672960,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 54, 815811)} (statscollectors.py:47)
[2022-03-09 11:12:22] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:23] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/7%C2%B725%E5%8D%B0%E5%BA%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:23] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:23] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/7%C2%B725%E5%8D%B0%E5%BA%A6%E5%B1%B1%E4%BD%93%E6%BB%91%E5%9D%A1%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:23] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:23] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1347,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 329.838544,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 23, 521893),
 'log_count/DEBUG': 466,
 'log_count/ERROR': 57,
 'log_count/INFO': 602,
 'memusage/max': 76165120,
 'memusage/startup': 69509120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 53, 683349)} (statscollectors.py:47)
[2022-03-09 11:12:23] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:23] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/12%C2%B714%E5%8D%B0%E5%BA%A6%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:23] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:23] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:23] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/12%C2%B714%E5%8D%B0%E5%BA%A6%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6 (Baidu.py:149)
[2022-03-09 11:12:23] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:23] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1354,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 319.835936,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 23, 739093),
 'log_count/DEBUG': 403,
 'log_count/ERROR': 60,
 'log_count/INFO': 534,
 'memusage/max': 76165120,
 'memusage/startup': 71282688,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 3, 903157)} (statscollectors.py:47)
[2022-03-09 11:12:23] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:24] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%97%E9%83%A8%E5%86%B0%E5%B7%9D%E6%96%AD%E8%A3%82> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%97%E9%83%A8%E5%86%B0%E5%B7%9D%E6%96%AD%E8%A3%82 (Baidu.py:149)
[2022-03-09 11:12:24] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:24] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1349,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 335.238919,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 24, 328115),
 'log_count/DEBUG': 494,
 'log_count/ERROR': 63,
 'log_count/INFO': 641,
 'memusage/max': 76165120,
 'memusage/startup': 68816896,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 49, 89196)} (statscollectors.py:47)
[2022-03-09 11:12:24] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:24] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2019%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%B4%AA%E7%81%BE> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2019%E5%B9%B4%E5%8D%B0%E5%BA%A6%E6%B4%AA%E7%81%BE (Baidu.py:149)
[2022-03-09 11:12:24] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:24] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1273,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 334.371283,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 24, 592479),
 'log_count/DEBUG': 487,
 'log_count/ERROR': 66,
 'log_count/INFO': 636,
 'memusage/max': 76165120,
 'memusage/startup': 69005312,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 50, 221196)} (statscollectors.py:47)
[2022-03-09 11:12:24] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:24] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/5%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%96%E5%B7%A5%E5%8E%82%E6%AF%92%E6%B0%94%E6%B3%84%E6%BC%8F%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:24] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/5%C2%B77%E5%8D%B0%E5%BA%A6%E5%8C%96%E5%B7%A5%E5%8E%82%E6%AF%92%E6%B0%94%E6%B3%84%E6%BC%8F%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:25] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1435,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 320.018017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 25, 85507),
 'log_count/DEBUG': 396,
 'log_count/ERROR': 69,
 'log_count/INFO': 535,
 'memusage/max': 76165120,
 'memusage/startup': 71479296,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 5, 67490)} (statscollectors.py:47)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:25] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:25] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BC%A0%E7%96%AB> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BC%A0%E7%96%AB (Baidu.py:149)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:25] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1223,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 327.353875,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 25, 547598),
 'log_count/DEBUG': 438,
 'log_count/ERROR': 72,
 'log_count/INFO': 587,
 'memusage/max': 76165120,
 'memusage/startup': 70275072,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 58, 193723)} (statscollectors.py:47)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:25] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8D%9A%E5%B8%95%E5%B0%94%E7%81%BE%E9%9A%BE> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:25] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/12%C2%B717%E5%8D%B0%E5%BA%A6%E5%AF%8C%E5%A3%AB%E5%BA%B7%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8D%9A%E5%B8%95%E5%B0%94%E7%81%BE%E9%9A%BE (Baidu.py:149)
[2022-03-09 11:12:25] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6H5N1%E7%A6%BD%E6%B5%81%E6%84%9F> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/12%C2%B717%E5%8D%B0%E5%BA%A6%E5%AF%8C%E5%A3%AB%E5%BA%B7%E9%A3%9F%E7%89%A9%E4%B8%AD%E6%AF%92%E4%BA%8B%E4%BB%B6 (Baidu.py:149)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:25] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1312,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 326.625425,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 25, 965364),
 'log_count/DEBUG': 431,
 'log_count/ERROR': 79,
 'log_count/INFO': 582,
 'memusage/max': 76165120,
 'memusage/startup': 70492160,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 59, 339939)} (statscollectors.py:47)
[2022-03-09 11:12:25] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:25] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6H5N1%E7%A6%BD%E6%B5%81%E6%84%9F (Baidu.py:149)
[2022-03-09 11:12:26] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:26] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1448,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 323.247326,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 26, 12108),
 'log_count/DEBUG': 410,
 'log_count/ERROR': 81,
 'log_count/INFO': 561,
 'memusage/max': 76165120,
 'memusage/startup': 71090176,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 2, 764782)} (statscollectors.py:47)
[2022-03-09 11:12:26] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:26] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:26] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1260,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 325.578033,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 26, 75432),
 'log_count/DEBUG': 424,
 'log_count/ERROR': 81,
 'log_count/INFO': 580,
 'memusage/max': 76165120,
 'memusage/startup': 70696960,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 0, 497399)} (statscollectors.py:47)
[2022-03-09 11:12:26] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:26] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:27] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E8%9D%97%E8%99%AB%E8%A2%AD%E5%87%BB%E6%96%B0%E5%BE%B7%E9%87%8C> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:27] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:27] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E8%9D%97%E8%99%AB%E8%A2%AD%E5%87%BB%E6%96%B0%E5%BE%B7%E9%87%8C (Baidu.py:149)
[2022-03-09 11:12:27] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:27] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1336,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 340.544414,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 27, 421434),
 'log_count/DEBUG': 508,
 'log_count/ERROR': 84,
 'log_count/INFO': 680,
 'memusage/max': 76165120,
 'memusage/startup': 68464640,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 6, 46, 877020)} (statscollectors.py:47)
[2022-03-09 11:12:27] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:28] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B5%B7%E5%86%9B%E8%88%AA%E9%81%93%E6%BC%94%E4%B9%A0> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:28] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:28] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E6%B5%B7%E5%86%9B%E8%88%AA%E9%81%93%E6%BC%94%E4%B9%A0 (Baidu.py:149)
[2022-03-09 11:12:28] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:28] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1349,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 317.389979,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 28, 839863),
 'log_count/DEBUG': 361,
 'log_count/ERROR': 87,
 'log_count/INFO': 516,
 'memusage/max': 76165120,
 'memusage/startup': 72450048,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 11, 449884)} (statscollectors.py:47)
[2022-03-09 11:12:28] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:28] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B7%A1%E8%88%AA%E5%AF%BC%E5%BC%B9%E8%AF%95%E5%B0%84%E5%A4%B1%E8%B4%A5> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:28] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:28] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B7%A1%E8%88%AA%E5%AF%BC%E5%BC%B9%E8%AF%95%E5%B0%84%E5%A4%B1%E8%B4%A5 (Baidu.py:149)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:29] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1382,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 313.257673,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 29, 60660),
 'log_count/DEBUG': 340,
 'log_count/ERROR': 90,
 'log_count/INFO': 496,
 'memusage/max': 76165120,
 'memusage/startup': 73052160,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 15, 802987)} (statscollectors.py:47)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:29] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%82%AE%E6%88%98> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:29] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:29] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%82%AE%E6%88%98 (Baidu.py:149)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:29] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1311,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 322.294162,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 29, 645079),
 'log_count/DEBUG': 382,
 'log_count/ERROR': 93,
 'log_count/INFO': 547,
 'memusage/max': 76165120,
 'memusage/startup': 71876608,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 7, 350917)} (statscollectors.py:47)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:29] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/1962%E5%B9%B4%E6%8C%91%E8%B5%B7%E4%B8%AD%E5%8D%B0%E8%BE%B9%E7%95%8C%E4%BA%89%E7%AB%AF> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:29] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:29] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/1962%E5%B9%B4%E6%8C%91%E8%B5%B7%E4%B8%AD%E5%8D%B0%E8%BE%B9%E7%95%8C%E4%BA%89%E7%AB%AF (Baidu.py:149)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:29] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1373,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 323.694794,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 29, 907120),
 'log_count/DEBUG': 389,
 'log_count/ERROR': 96,
 'log_count/INFO': 558,
 'memusage/max': 76165120,
 'memusage/startup': 71663616,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 6, 212326)} (statscollectors.py:47)
[2022-03-09 11:12:29] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:30] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E6%B3%95%E5%9B%BD%E9%98%B5%E9%A3%8E%E6%88%98%E6%9C%BA> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:30] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:30] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E6%B3%95%E5%9B%BD%E9%98%B5%E9%A3%8E%E6%88%98%E6%9C%BA (Baidu.py:149)
[2022-03-09 11:12:30] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:30] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1408,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 317.798169,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 30, 403239),
 'log_count/DEBUG': 354,
 'log_count/ERROR': 99,
 'log_count/INFO': 521,
 'memusage/max': 76165120,
 'memusage/startup': 72642560,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 12, 605070)} (statscollectors.py:47)
[2022-03-09 11:12:30] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:30] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:35] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:35] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%86%B2%E7%AA%81> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%B7%B4%E5%86%B2%E7%AA%81 (Baidu.py:149)
[2022-03-09 11:12:35] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:35] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1225,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 326.533617,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 35, 553593),
 'log_count/DEBUG': 375,
 'log_count/ERROR': 102,
 'log_count/INFO': 551,
 'memusage/max': 76165120,
 'memusage/startup': 72065024,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 9, 19976)} (statscollectors.py:47)
[2022-03-09 11:12:35] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:35] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9B%E4%BA%8B%E6%BC%94%E4%B9%A0> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:35] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E7%BE%8E%E5%9B%BDF-16%E6%88%98%E6%9C%BA> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9B%E4%BA%8B%E6%BC%94%E4%B9%A0 (Baidu.py:149)
[2022-03-09 11:12:35] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E6%96%B0%E5%9E%8B%E5%8F%8D%E9%9B%B7%E8%BE%BE%E5%AF%BC%E5%BC%B9> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B4%AD%E4%B9%B0%E7%BE%8E%E5%9B%BDF-16%E6%88%98%E6%9C%BA (Baidu.py:149)
[2022-03-09 11:12:35] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:35] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1269,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 325.673037,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 35, 973439),
 'log_count/DEBUG': 368,
 'log_count/ERROR': 109,
 'log_count/INFO': 546,
 'memusage/max': 76165120,
 'memusage/startup': 72257536,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 10, 300402)} (statscollectors.py:47)
[2022-03-09 11:12:35] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:35] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E6%96%B0%E5%9E%8B%E5%8F%8D%E9%9B%B7%E8%BE%BE%E5%AF%BC%E5%BC%B9 (Baidu.py:149)
[2022-03-09 11:12:36] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:36] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1338,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 322.006417,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 36, 22463),
 'log_count/DEBUG': 347,
 'log_count/ERROR': 111,
 'log_count/INFO': 525,
 'memusage/max': 76165120,
 'memusage/startup': 72851456,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 14, 16046)} (statscollectors.py:47)
[2022-03-09 11:12:36] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:36] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:36] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1413,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 317.674228,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 36, 77596),
 'log_count/DEBUG': 333,
 'log_count/ERROR': 111,
 'log_count/INFO': 513,
 'memusage/max': 76165120,
 'memusage/startup': 73265152,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 18, 403368)} (statscollectors.py:47)
[2022-03-09 11:12:36] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:37] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E5%8F%AF%E8%BD%BD%E6%A0%B8%E5%BC%B9%E5%A4%B4%E5%AF%BC%E5%BC%B9> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:37] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:37] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84%E5%8F%AF%E8%BD%BD%E6%A0%B8%E5%BC%B9%E5%A4%B4%E5%AF%BC%E5%BC%B9 (Baidu.py:149)
[2022-03-09 11:12:37] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:37] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1399,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 317.820585,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 37, 431317),
 'log_count/DEBUG': 326,
 'log_count/ERROR': 114,
 'log_count/INFO': 509,
 'memusage/max': 76165120,
 'memusage/startup': 73437184,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 19, 610732)} (statscollectors.py:47)
[2022-03-09 11:12:37] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:37] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:38] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8C%BB%E6%8A%A4%E9%81%87%E8%A2%AD> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:38] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:38] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%8C%BB%E6%8A%A4%E9%81%87%E8%A2%AD (Baidu.py:149)
[2022-03-09 11:12:38] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:38] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1272,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 309.926943,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 38, 849782),
 'log_count/DEBUG': 291,
 'log_count/ERROR': 117,
 'log_count/INFO': 473,
 'memusage/max': 76165120,
 'memusage/startup': 74473472,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 28, 922839)} (statscollectors.py:47)
[2022-03-09 11:12:38] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:38] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84K-4%E6%BD%9C%E5%B0%84%E5%AF%BC%E5%BC%B9> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:38] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:38] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%AF%95%E5%B0%84K-4%E6%BD%9C%E5%B0%84%E5%AF%BC%E5%BC%B9 (Baidu.py:149)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1334,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 318.076552,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 69925),
 'log_count/DEBUG': 319,
 'log_count/ERROR': 120,
 'log_count/INFO': 508,
 'memusage/max': 76165120,
 'memusage/startup': 73654272,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 20, 993373)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:39] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BB%91%E5%85%AC%E4%BA%A4%E8%BD%AE%E5%A5%B8%E6%A1%88> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%BB%91%E5%85%AC%E4%BA%A4%E8%BD%AE%E5%A5%B8%E6%A1%88 (Baidu.py:149)
[2022-03-09 11:12:39] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B0%91%E5%A5%B3%E8%A2%AB%E6%B3%BC%E6%B1%BD%E6%B2%B9%E7%83%A7%E6%AD%BB> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1349,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 305.286132,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 656330),
 'log_count/DEBUG': 277,
 'log_count/ERROR': 124,
 'log_count/INFO': 463,
 'memusage/max': 76165120,
 'memusage/startup': 74874880,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 34, 370198)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:39] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%AD%9F%E4%B9%B0%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:39] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B8%A9%E8%B8%8F%E4%BA%8B%E4%BB%B6> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:39] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%B0%91%E5%A5%B3%E8%A2%AB%E6%B3%BC%E6%B1%BD%E6%B2%B9%E7%83%A7%E6%AD%BB (Baidu.py:149)
[2022-03-09 11:12:39] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%BC%BA%E5%A5%B8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%AD%9F%E4%B9%B0%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB (Baidu.py:149)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E8%B8%A9%E8%B8%8F%E4%BA%8B%E4%BB%B6 (Baidu.py:149)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1388,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 313.270544,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 857923),
 'log_count/DEBUG': 298,
 'log_count/ERROR': 133,
 'log_count/INFO': 491,
 'memusage/max': 76165120,
 'memusage/startup': 74264576,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 26, 587379)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:39] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%BC%BA%E5%A5%B8 (Baidu.py:149)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1277,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 302.079183,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 903737),
 'log_count/DEBUG': 263,
 'log_count/ERROR': 135,
 'log_count/INFO': 454,
 'memusage/max': 76165120,
 'memusage/startup': 75284480,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 37, 824554)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1280,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 303.872361,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 917312),
 'log_count/DEBUG': 270,
 'log_count/ERROR': 135,
 'log_count/INFO': 465,
 'memusage/max': 76165120,
 'memusage/startup': 75096064,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 36, 44951)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:39] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1237,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 309.192127,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 39, 967416),
 'log_count/DEBUG': 284,
 'log_count/ERROR': 135,
 'log_count/INFO': 484,
 'memusage/max': 76165120,
 'memusage/startup': 74670080,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 30, 775289)} (statscollectors.py:47)
[2022-03-09 11:12:39] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:40] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/6%C2%B715%E4%B8%AD%E5%8D%B0%E5%8A%A0%E5%8B%92%E4%B8%87%E6%B2%B3%E8%B0%B7%E8%BE%B9%E5%A2%83%E5%86%B2%E7%AA%81> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:40] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:40] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/6%C2%B715%E4%B8%AD%E5%8D%B0%E5%8A%A0%E5%8B%92%E4%B8%87%E6%B2%B3%E8%B0%B7%E8%BE%B9%E5%A2%83%E5%86%B2%E7%AA%81 (Baidu.py:149)
[2022-03-09 11:12:40] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:40] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1455,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 316.852339,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 40, 411011),
 'log_count/DEBUG': 312,
 'log_count/ERROR': 138,
 'log_count/INFO': 519,
 'memusage/max': 76165120,
 'memusage/startup': 73842688,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 23, 558672)} (statscollectors.py:47)
[2022-03-09 11:12:40] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:40] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E4%B8%AD%E5%8D%B0%E4%B8%BE%E8%A1%8C%E5%86%9B%E9%95%BF%E7%BA%A7%E4%BC%9A%E6%99%A4> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:40] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:40] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E4%B8%AD%E5%8D%B0%E4%B8%BE%E8%A1%8C%E5%86%9B%E9%95%BF%E7%BA%A7%E4%BC%9A%E6%99%A4 (Baidu.py:149)
[2022-03-09 11:12:40] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:40] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1355,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 315.713688,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 40, 886487),
 'log_count/DEBUG': 305,
 'log_count/ERROR': 141,
 'log_count/INFO': 514,
 'memusage/max': 76165120,
 'memusage/startup': 74072064,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 25, 172799)} (statscollectors.py:47)
[2022-03-09 11:12:40] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:42] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/7%C2%B72%E5%8D%B0%E5%BA%A6%E6%B0%B4%E5%9D%9D%E6%BA%83%E5%A0%A4%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:42] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:42] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:42] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/7%C2%B72%E5%8D%B0%E5%BA%A6%E6%B0%B4%E5%9D%9D%E6%BA%83%E5%A0%A4%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:42] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:42] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1353,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 219.061383,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 42, 759326),
 'log_count/DEBUG': 221,
 'log_count/ERROR': 144,
 'log_count/INFO': 423,
 'memusage/max': 76165120,
 'memusage/startup': 76075008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 3, 697943)} (statscollectors.py:47)
[2022-03-09 11:12:42] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:43] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:43] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/5%C2%B722%E5%8D%B0%E5%BA%A6%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%9D%A0%E6%9C%BA%E4%BA%8B%E4%BB%B6> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:43] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:43] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/5%C2%B722%E5%8D%B0%E5%BA%A6%E8%88%AA%E7%A9%BA%E5%85%AC%E5%8F%B8%E5%9D%A0%E6%9C%BA%E4%BA%8B%E4%BB%B6 (Baidu.py:149)
[2022-03-09 11:12:43] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:43] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 239.347505,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 43, 802071),
 'log_count/DEBUG': 235,
 'log_count/ERROR': 147,
 'log_count/INFO': 443,
 'memusage/max': 76165120,
 'memusage/startup': 76075008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 8, 44, 454566)} (statscollectors.py:47)
[2022-03-09 11:12:43] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:43] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/10%C2%B73%E5%8D%B0%E5%BA%A6%E5%B7%B4%E7%89%B9%E9%82%A3%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A%E8%B8%A9%E8%B8%8F%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/10%C2%B73%E5%8D%B0%E5%BA%A6%E5%B7%B4%E7%89%B9%E9%82%A3%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A%E8%B8%A9%E8%B8%8F%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:44] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E7%A4%BA%E5%A8%81%E6%B8%B8%E8%A1%8C> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:44] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1491,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 218.172555,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 44, 192919),
 'log_count/DEBUG': 214,
 'log_count/ERROR': 151,
 'log_count/INFO': 422,
 'memusage/max': 76165120,
 'memusage/startup': 76075008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 6, 20364)} (statscollectors.py:47)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E7%A4%BA%E5%A8%81%E6%B8%B8%E8%A1%8C (Baidu.py:149)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:44] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1284,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 183.397908,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 44, 393837),
 'log_count/DEBUG': 193,
 'log_count/ERROR': 153,
 'log_count/INFO': 401,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 40, 995929)} (statscollectors.py:47)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:44] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/12%C2%B727%E5%8D%B0%E5%BA%A6%E5%AE%A2%E6%9C%BA%E5%86%B2%E5%87%BA%E8%B7%91%E9%81%93%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:44] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/12%C2%B727%E5%8D%B0%E5%BA%A6%E5%AE%A2%E6%9C%BA%E5%86%B2%E5%87%BA%E8%B7%91%E9%81%93%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:44] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1427,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 305.235991,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 44, 983095),
 'log_count/DEBUG': 256,
 'log_count/ERROR': 156,
 'log_count/INFO': 476,
 'memusage/max': 76165120,
 'memusage/startup': 75493376,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 39, 747104)} (statscollectors.py:47)
[2022-03-09 11:12:44] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:44] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2%C2%B712%E5%8D%B0%E5%BA%A6%E5%A4%A7%E5%B7%B4%E4%B8%8E%E5%8D%A1%E8%BD%A6%E7%9B%B8%E6%92%9E%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:45] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/9%C2%B77%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E6%B3%95%E9%99%A2%E7%88%86%E7%82%B8%E6%A1%88> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:45] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/8%C2%B728%E5%8D%B0%E5%BA%A6%E8%B4%BE%E5%9D%8E%E5%BE%B7%E9%82%A6%E5%A4%A7%E5%9D%9D%E5%9D%8D%E5%A1%8C%E4%BA%8B%E6%95%85> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2%C2%B712%E5%8D%B0%E5%BA%A6%E5%A4%A7%E5%B7%B4%E4%B8%8E%E5%8D%A1%E8%BD%A6%E7%9B%B8%E6%92%9E%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/9%C2%B77%E5%8D%B0%E5%BA%A6%E6%96%B0%E5%BE%B7%E9%87%8C%E6%B3%95%E9%99%A2%E7%88%86%E7%82%B8%E6%A1%88 (Baidu.py:149)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/8%C2%B728%E5%8D%B0%E5%BA%A6%E8%B4%BE%E5%9D%8E%E5%BE%B7%E9%82%A6%E5%A4%A7%E5%9D%9D%E5%9D%8D%E5%A1%8C%E4%BA%8B%E6%95%85 (Baidu.py:149)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:45] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1438,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 302.599847,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 45, 200244),
 'log_count/DEBUG': 249,
 'log_count/ERROR': 165,
 'log_count/INFO': 471,
 'memusage/max': 76165120,
 'memusage/startup': 75685888,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 7, 42, 600397)} (statscollectors.py:47)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:45] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1408,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 250.130207,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 45, 246297),
 'log_count/DEBUG': 242,
 'log_count/ERROR': 165,
 'log_count/INFO': 466,
 'memusage/max': 76165120,
 'memusage/startup': 75902976,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 8, 35, 116090)} (statscollectors.py:47)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:45] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1472,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 231.344538,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 45, 294489),
 'log_count/DEBUG': 228,
 'log_count/ERROR': 165,
 'log_count/INFO': 453,
 'memusage/max': 76165120,
 'memusage/startup': 76075008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 8, 53, 949951)} (statscollectors.py:47)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:45] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2%C2%B714%E5%8D%B0%E5%BA%A6%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:45] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2%C2%B714%E5%8D%B0%E5%BA%A6%E6%81%90%E6%80%96%E8%A2%AD%E5%87%BB%E4%BA%8B%E4%BB%B6 (Baidu.py:149)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:45] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1351,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 217.544779,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 45, 752612),
 'log_count/DEBUG': 207,
 'log_count/ERROR': 168,
 'log_count/INFO': 432,
 'memusage/max': 76165120,
 'memusage/startup': 76075008,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 8, 207833)} (statscollectors.py:47)
[2022-03-09 11:12:45] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:46] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9C%E6%B0%91%E8%BF%90%E5%8A%A8> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:46] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:46] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E5%86%9C%E6%B0%91%E8%BF%90%E5%8A%A8 (Baidu.py:149)
[2022-03-09 11:12:46] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:46] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1294,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 204.443745,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 46, 205304),
 'log_count/DEBUG': 200,
 'log_count/ERROR': 171,
 'log_count/INFO': 427,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 21, 761559)} (statscollectors.py:47)
[2022-03-09 11:12:46] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:47] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E4%B8%87%E4%BA%BA%E4%BC%8A%E6%96%AF%E5%85%B0%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:47] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:47] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E4%B8%87%E4%BA%BA%E4%BC%8A%E6%96%AF%E5%85%B0%E5%AE%97%E6%95%99%E9%9B%86%E4%BC%9A (Baidu.py:149)
[2022-03-09 11:12:48] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:48] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1370,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 175.385537,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 48, 75975),
 'log_count/DEBUG': 179,
 'log_count/ERROR': 174,
 'log_count/INFO': 406,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 52, 690438)} (statscollectors.py:47)
[2022-03-09 11:12:48] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:48] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/1%C2%B726%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%97%A5%E6%8A%97%E8%AE%AE%E5%86%B2%E7%AA%81> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/1%C2%B726%E5%8D%B0%E5%BA%A6%E5%85%B1%E5%92%8C%E5%9B%BD%E6%97%A5%E6%8A%97%E8%AE%AE%E5%86%B2%E7%AA%81 (Baidu.py:149)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:49] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1434,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 168.683432,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 49, 117570),
 'log_count/DEBUG': 165,
 'log_count/ERROR': 177,
 'log_count/INFO': 393,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 10, 0, 434138)} (statscollectors.py:47)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:49] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:12:49] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E5%8C%BB%E7%94%9F%E6%B8%B8%E8%A1%8C> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E5%8C%BB%E7%94%9F%E6%B8%B8%E8%A1%8C (Baidu.py:149)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:49] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1333,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 180.404993,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 49, 523361),
 'log_count/DEBUG': 186,
 'log_count/ERROR': 180,
 'log_count/INFO': 421,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 49, 118368)} (statscollectors.py:47)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:49] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/8%C2%B725%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:49] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/8%C2%B725%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1 (Baidu.py:149)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:49] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1248,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 172.828845,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 49, 739932),
 'log_count/DEBUG': 172,
 'log_count/ERROR': 183,
 'log_count/INFO': 408,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 9, 56, 911087)} (statscollectors.py:47)
[2022-03-09 11:12:49] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:50] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/9%C2%B72%E5%8D%B0%E5%BA%A6%E5%A4%A7%E7%BD%A2%E5%B7%A5> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/9%C2%B72%E5%8D%B0%E5%BA%A6%E5%A4%A7%E7%BD%A2%E5%B7%A5 (Baidu.py:149)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:50] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1277,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 161.490587,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 50, 318429),
 'log_count/DEBUG': 158,
 'log_count/ERROR': 186,
 'log_count/INFO': 395,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 10, 8, 827842)} (statscollectors.py:47)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:50] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/12%C2%B712%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:50] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/12%C2%B715%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:50] [   ERROR] [scrapy.downloadermiddlewares.retry ] - Gave up retrying <GET https://baike.baidu.com/item/2%C2%B724%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1> (failed 3 times): DNS lookup failed: http_proxy. (retry.py:122)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/12%C2%B712%E5%8D%B0%E5%BA%A6%E9%AA%9A%E4%B9%B1 (Baidu.py:149)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/12%C2%B715%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1 (Baidu.py:149)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - <twisted.python.failure.Failure twisted.internet.error.DNSLookupError: DNS lookup failed: http_proxy.> (Baidu.py:135)
[2022-03-09 11:12:50] [   ERROR] [     baidu ] - DNSLookupError on https://baike.baidu.com/item/2%C2%B724%E5%8D%B0%E5%BA%A6%E9%A6%96%E9%83%BD%E9%AA%9A%E4%B9%B1 (Baidu.py:149)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:50] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1253,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 142.547207,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 50, 551416),
 'log_count/DEBUG': 151,
 'log_count/ERROR': 195,
 'log_count/INFO': 390,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 10, 28, 4209)} (statscollectors.py:47)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:50] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1321,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 134.3736,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 50, 581061),
 'log_count/DEBUG': 144,
 'log_count/ERROR': 195,
 'log_count/INFO': 385,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 10, 36, 207461)} (statscollectors.py:47)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Closing spider (finished) (engine.py:309)
[2022-03-09 11:12:50] [    INFO] [scrapy.statscollectors ] - Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 1302,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'elapsed_time_seconds': 127.105409,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2022, 3, 9, 11, 12, 50, 630131),
 'log_count/DEBUG': 137,
 'log_count/ERROR': 195,
 'log_count/INFO': 380,
 'memusage/max': 76165120,
 'memusage/startup': 76165120,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2022, 3, 9, 11, 10, 43, 524722)} (statscollectors.py:47)
[2022-03-09 11:12:50] [    INFO] [scrapy.core.engine ] - Spider closed (finished) (engine.py:340)
[2022-03-09 11:12:50] [    INFO] [DataCleaning ] - 本次清洗用时：0:00:00.000438 (DataCleaning.py:41)
[2022-03-09 11:12:50] [    INFO] [  __main__ ] - upload crawl file success (MultisiteSchedule.py:395)
[2022-03-09 11:12:50] [   ERROR] [  __main__ ] - [Errno 2] No such file or directory: 'result/url' (MultisiteSchedule.py:261)
Traceback (most recent call last):
  File "MultisiteSchedule.py", line 235, in upload_crawl_img_new
    img_file_list = os.listdir(file_index)
FileNotFoundError: [Errno 2] No such file or directory: 'result/url'
[2022-03-09 11:12:50] [    INFO] [  __main__ ] - upload crawl file failure (MultisiteSchedule.py:398)
[2022-03-09 11:12:50] [    INFO] [  __main__ ] - scrapy finished (MultisiteSchedule.py:403)
[2022-03-09 11:46:52] [    INFO] [  __main__ ] - TextCrawler On! (MultisiteSchedule.py:373)
[2022-03-09 11:46:52] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:52] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:46:53] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33253/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:46:53] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:33253 (connectionpool.py:232)
[2022-03-09 11:46:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33253 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:46:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33253/session/c8a2926595dca640a86c7adfd55337d3/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:46:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33253 "POST /session/c8a2926595dca640a86c7adfd55337d3/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:46:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:54] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:46:54] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:46:54] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:46:54] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:46:54] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:46:54] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:46:54] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:54] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:46:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51311/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:46:55] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51311 (connectionpool.py:232)
[2022-03-09 11:46:56] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51311 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:46:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51311/session/fbcbcdb20ae3863aeeb816842c03c899/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:46:56] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51311 "POST /session/fbcbcdb20ae3863aeeb816842c03c899/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:46:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:56] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:46:56] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:46:56] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:46:56] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:46:56] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:46:56] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:46:56] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:56] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:46:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38647/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:46:57] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38647 (connectionpool.py:232)
[2022-03-09 11:46:57] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38647 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:46:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38647/session/de03a797921f800a575721f657ff9d98/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:46:57] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38647 "POST /session/de03a797921f800a575721f657ff9d98/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:46:57] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:57] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:46:57] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:46:57] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:46:57] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:46:57] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:46:57] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:46:57] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:57] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:46:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39583/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:46:58] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39583 (connectionpool.py:232)
[2022-03-09 11:46:58] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39583 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:46:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39583/session/11035e8cebf5f899039111788d7ca653/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:46:58] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39583 "POST /session/11035e8cebf5f899039111788d7ca653/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:46:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:58] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:46:58] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:46:58] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:46:58] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:46:58] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:46:58] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:46:58] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:58] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:46:58] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:32919/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:46:58] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:32919 (connectionpool.py:232)
[2022-03-09 11:46:59] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:32919 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:46:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:32919/session/b136d3c27f82f9aa0869dc196792e290/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:46:59] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:32919 "POST /session/b136d3c27f82f9aa0869dc196792e290/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:46:59] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:46:59] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:46:59] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:46:59] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:46:59] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:46:59] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:46:59] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:46:59] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:46:59] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:00] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:52777/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:00] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:52777 (connectionpool.py:232)
[2022-03-09 11:47:02] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:52777 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:52777/session/b1546465a4585c7af17ca4ca3e17c636/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:02] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:52777 "POST /session/b1546465a4585c7af17ca4ca3e17c636/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:02] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:02] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:02] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:02] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:02] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:02] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:02] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:02] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:02] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:03] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53179/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:03] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:53179 (connectionpool.py:232)
[2022-03-09 11:47:04] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53179 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53179/session/4b22eeaeecffeae6ca2b2baa719c7e9a/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:04] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53179 "POST /session/4b22eeaeecffeae6ca2b2baa719c7e9a/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:04] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:04] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:04] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:04] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:04] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:04] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:04] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:04] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:05] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37997/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:05] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37997 (connectionpool.py:232)
[2022-03-09 11:47:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37997 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37997/session/1b83ac1bc5730c5a13c787856bfebcbb/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37997 "POST /session/1b83ac1bc5730c5a13c787856bfebcbb/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:06] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:06] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:06] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:06] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:06] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:06] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:06] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:06] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:06] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:54151/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:07] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:54151 (connectionpool.py:232)
[2022-03-09 11:47:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:54151 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:54151/session/e06144797d01f181e1d51246be4f2002/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:54151 "POST /session/e06144797d01f181e1d51246be4f2002/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:08] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:08] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:08] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:08] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:08] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:08] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:08] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:08] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39815/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:09] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39815 (connectionpool.py:232)
[2022-03-09 11:47:09] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39815 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39815/session/027422f1b2bc8fba1055efa214cefb9f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:09] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39815 "POST /session/027422f1b2bc8fba1055efa214cefb9f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:09] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:09] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:09] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:09] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:09] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:09] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:09] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:09] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:09] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:10] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36571/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:10] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36571 (connectionpool.py:232)
[2022-03-09 11:47:11] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36571 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36571/session/ae3a6c950e6c4e48a69cd1043009808a/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:11] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36571 "POST /session/ae3a6c950e6c4e48a69cd1043009808a/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:11] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:11] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:11] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:11] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:11] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:11] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:11] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:11] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49729/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:12] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:49729 (connectionpool.py:232)
[2022-03-09 11:47:12] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49729 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:49729/session/f93f1b0da4c44f0490408b0a89593165/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:12] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:49729 "POST /session/f93f1b0da4c44f0490408b0a89593165/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:12] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:12] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:12] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:12] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:12] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:12] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:12] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:12] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:13] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55503/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:13] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55503 (connectionpool.py:232)
[2022-03-09 11:47:14] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55503 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55503/session/3fbb3ec4afe470c288e9074c12076a92/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:14] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55503 "POST /session/3fbb3ec4afe470c288e9074c12076a92/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:14] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:14] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:14] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:14] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:14] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:14] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:14] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:14] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:14] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33881/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:15] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:33881 (connectionpool.py:232)
[2022-03-09 11:47:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33881 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:33881/session/63c6f2b09e64ccf1224efc6fb93bf9d7/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:33881 "POST /session/63c6f2b09e64ccf1224efc6fb93bf9d7/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:15] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:15] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:15] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:15] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:15] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:15] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:15] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:15] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:16] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46901/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:16] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:46901 (connectionpool.py:232)
[2022-03-09 11:47:17] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46901 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:17] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:17] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46901/session/e1da37a2381f7249cd16a56db253024b/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:17] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46901 "POST /session/e1da37a2381f7249cd16a56db253024b/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:17] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:17] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:17] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:17] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:17] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:17] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:17] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:17] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:17] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46189/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:18] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:46189 (connectionpool.py:232)
[2022-03-09 11:47:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46189 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:46189/session/b812504a15833f3950ee3ae62a977d17/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:46189 "POST /session/b812504a15833f3950ee3ae62a977d17/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:18] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:18] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:18] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:18] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:18] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:18] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:18] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:18] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60183/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:19] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:60183 (connectionpool.py:232)
[2022-03-09 11:47:20] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60183 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60183/session/f69311030dedef2732f0ee6ab0fcb781/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:20] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60183 "POST /session/f69311030dedef2732f0ee6ab0fcb781/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:20] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:20] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:20] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:20] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:20] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:20] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:20] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:20] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:20] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59835/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:21] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:59835 (connectionpool.py:232)
[2022-03-09 11:47:22] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59835 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:22] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:22] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59835/session/fb007979855f89d38f8f057ce20ace9d/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:22] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59835 "POST /session/fb007979855f89d38f8f057ce20ace9d/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:22] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:22] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:22] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:22] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:22] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:22] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:22] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:22] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:22] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44805/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:23] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:44805 (connectionpool.py:232)
[2022-03-09 11:47:23] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44805 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44805/session/888be314453f3f55857b52b352f74bf3/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:23] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44805 "POST /session/888be314453f3f55857b52b352f74bf3/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:23] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:23] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:23] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:23] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:23] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:23] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:23] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:23] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:23] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36379/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:24] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36379 (connectionpool.py:232)
[2022-03-09 11:47:24] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36379 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36379/session/f8c72a5733cdc678b674f3664df83786/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:24] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36379 "POST /session/f8c72a5733cdc678b674f3664df83786/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:24] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:24] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:24] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:24] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:24] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:24] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:24] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:24] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:24] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:25] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47403/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:25] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:47403 (connectionpool.py:232)
[2022-03-09 11:47:26] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47403 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47403/session/4a690108000812213b64676d5954e3ca/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:26] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47403 "POST /session/4a690108000812213b64676d5954e3ca/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:26] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:26] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:26] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:26] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:26] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:26] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:26] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:26] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41837/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:27] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:41837 (connectionpool.py:232)
[2022-03-09 11:47:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41837 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:41837/session/0b439a3aa1aab8d77b5079e801729c73/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:41837 "POST /session/0b439a3aa1aab8d77b5079e801729c73/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:27] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:27] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:27] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:27] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:27] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:27] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:27] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:27] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55719/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:28] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55719 (connectionpool.py:232)
[2022-03-09 11:47:28] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55719 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55719/session/2b5b24bfa4c515fe8ba146dfb0c0b46d/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:28] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55719 "POST /session/2b5b24bfa4c515fe8ba146dfb0c0b46d/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:28] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:28] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:28] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:28] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:28] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:28] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:28] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:28] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:28] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39209/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:29] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39209 (connectionpool.py:232)
[2022-03-09 11:47:29] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39209 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39209/session/ab5e6d277ad070ff605f4432fd180f8f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:29] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39209 "POST /session/ab5e6d277ad070ff605f4432fd180f8f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:29] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:29] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:29] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:29] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:29] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:29] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:29] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:29] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:30] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39391/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:30] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:39391 (connectionpool.py:232)
[2022-03-09 11:47:31] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39391 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:39391/session/64325146997d569618f379b1c41eaff6/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:31] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:39391 "POST /session/64325146997d569618f379b1c41eaff6/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:31] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:31] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:31] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:31] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:31] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:31] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:31] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:31] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:32] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35239/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:32] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:35239 (connectionpool.py:232)
[2022-03-09 11:47:33] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35239 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:33] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:33] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:35239/session/f38d3ba65e6a3c96bc54742c58876c38/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:33] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:35239 "POST /session/f38d3ba65e6a3c96bc54742c58876c38/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:33] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:33] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:33] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:33] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:33] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:33] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:33] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:33] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:33] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:42891/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:34] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:42891 (connectionpool.py:232)
[2022-03-09 11:47:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:42891 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:42891/session/e162a9fa549ee7d7f1f085486ce4a796/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:34] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:42891 "POST /session/e162a9fa549ee7d7f1f085486ce4a796/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:34] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:34] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:34] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:34] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:34] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:34] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:34] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:34] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:35] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59739/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:35] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:59739 (connectionpool.py:232)
[2022-03-09 11:47:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59739 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59739/session/b2b221c596fa1ffc3e0744af6fdd3f68/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59739 "POST /session/b2b221c596fa1ffc3e0744af6fdd3f68/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:36] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:36] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:36] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:36] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:36] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:36] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:36] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:36] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58899/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:37] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58899 (connectionpool.py:232)
[2022-03-09 11:47:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58899 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58899/session/be34a7b485c1e514f90a894dad91c60c/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:37] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58899 "POST /session/be34a7b485c1e514f90a894dad91c60c/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:37] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:37] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:37] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:37] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:37] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:38] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:38] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:38] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:38] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47869/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:39] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:47869 (connectionpool.py:232)
[2022-03-09 11:47:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47869 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47869/session/24a2baa09deb8715dfd9acef6bd64ca0/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:40] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47869 "POST /session/24a2baa09deb8715dfd9acef6bd64ca0/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:40] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:40] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:40] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:40] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:40] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:40] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:40] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:40] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:40] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:54609/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:41] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:54609 (connectionpool.py:232)
[2022-03-09 11:47:46] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:54609 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:54609/session/28ec73c7818a7b05408111725ad001b6/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:47] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:54609 "POST /session/28ec73c7818a7b05408111725ad001b6/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:47] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:47] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:47] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:47] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:47] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:47] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:47] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:47] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:48] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51579/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:48] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:51579 (connectionpool.py:232)
[2022-03-09 11:47:55] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51579 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:47:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:51579/session/5ea0c3ab62384eb692ede3b7981c089f/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:47:55] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:51579 "POST /session/5ea0c3ab62384eb692ede3b7981c089f/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:47:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:47:55] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:47:55] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:47:55] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:47:55] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:47:55] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:47:55] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:47:55] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:47:55] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:47:56] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50925/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:47:56] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:50925 (connectionpool.py:232)
[2022-03-09 11:48:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50925 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:48:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:48:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:50925/session/39ed2c3621d3c1800793b4b521eb0e60/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:48:15] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:50925 "POST /session/39ed2c3621d3c1800793b4b521eb0e60/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:48:15] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:48:15] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:48:15] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:48:15] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:48:15] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:48:15] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:48:15] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:48:15] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:48:15] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:48:16] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:52255/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:48:16] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:52255 (connectionpool.py:232)
[2022-03-09 11:49:17] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:52255 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 11:49:17] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:49:18] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 11:49:18] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 11:49:19] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:49:19] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:49:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60377/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:49:21] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:60377 (connectionpool.py:232)
[2022-03-09 11:49:48] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60377 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:49:48] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:49:48] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:60377/session/733ea06eb09d6ecca1bd8e254d536683/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:49:48] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:60377 "POST /session/733ea06eb09d6ecca1bd8e254d536683/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:49:48] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:49:48] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:49:48] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:49:48] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:49:48] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:49:48] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:49:48] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:49:48] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:49:48] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:49:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59809/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:49:49] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:59809 (connectionpool.py:232)
[2022-03-09 11:49:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59809 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:49:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:49:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59809/session/4962aa04362503b45c54f7b4361cee90/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:49:54] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59809 "POST /session/4962aa04362503b45c54f7b4361cee90/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:49:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:49:54] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:49:54] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:49:54] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:49:54] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:49:54] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:49:54] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:49:54] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:49:54] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:49:55] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59681/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:49:55] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:59681 (connectionpool.py:232)
[2022-03-09 11:50:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59681 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:50:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:50:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:59681/session/e972b33c7f36c5728a3edb9b77080fe4/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:50:49] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:59681 "POST /session/e972b33c7f36c5728a3edb9b77080fe4/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:50:49] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:50:49] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:50:49] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:50:50] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:50:50] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:50:50] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:50:50] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:50:50] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:50:50] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:50:51] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:43341/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:50:51] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:43341 (connectionpool.py:232)
[2022-03-09 11:52:29] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:43341 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 11:52:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:52:30] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 11:52:30] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 11:52:30] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:52:30] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:52:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38109/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:52:31] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:38109 (connectionpool.py:232)
[2022-03-09 11:53:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38109 "POST /session HTTP/1.1" 200 753 (connectionpool.py:465)
[2022-03-09 11:53:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:53:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:38109/session/1e2248e998768959a0aa94d5832913a9/timeouts {"implicit": 1000} (remote_connection.py:388)
[2022-03-09 11:53:18] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:38109 "POST /session/1e2248e998768959a0aa94d5832913a9/timeouts HTTP/1.1" 200 14 (connectionpool.py:465)
[2022-03-09 11:53:18] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:53:18] [    INFO] [scrapy.middleware ] - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats'] (middleware.py:48)
[2022-03-09 11:53:18] [    INFO] [scrapy.middleware ] - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware'] (middleware.py:48)
[2022-03-09 11:53:18] [    INFO] [scrapy.middleware ] - Enabled item pipelines:
['TextProcessorScrapy.pipelines.ImagePipeline',
 'TextProcessorScrapy.pipelines.BaiduPipeline'] (middleware.py:48)
[2022-03-09 11:53:18] [    INFO] [scrapy.core.engine ] - Spider opened (engine.py:272)
[2022-03-09 11:53:18] [    INFO] [scrapy.extensions.logstats ] - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) (logstats.py:48)
[2022-03-09 11:53:18] [    INFO] [     baidu ] - Spider opened: baidu (middlewares.py:142)
[2022-03-09 11:53:18] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:53:18] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:53:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:56677/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:53:19] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:56677 (connectionpool.py:232)
[2022-03-09 11:54:21] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:56677 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 11:54:21] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:54:23] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 11:54:23] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 11:54:24] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:54:24] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:54:46] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:56519/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:54:47] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:56519 (connectionpool.py:232)
[2022-03-09 11:57:06] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:56519 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 11:57:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:57:09] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:57:10] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:57:13] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55779/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:57:13] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55779 (connectionpool.py:232)
[2022-03-09 11:58:22] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55779 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 11:58:22] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:58:23] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 11:58:23] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 11:58:23] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:58:23] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:58:25] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37821/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:58:25] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37821 (connectionpool.py:232)
[2022-03-09 11:59:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:37821 "POST /session HTTP/1.1" 500 746 (connectionpool.py:465)
[2022-03-09 11:59:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 11:59:27] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 11:59:28] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 11:59:39] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40645/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 11:59:39] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40645 (connectionpool.py:232)
[2022-03-09 12:01:29] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40645 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:01:54] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:02:00] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:02:00] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:02:01] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:02:01] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:02:04] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:53539/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:02:04] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:53539 (connectionpool.py:232)
[2022-03-09 12:03:26] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:53539 "POST /session HTTP/1.1" 500 746 (connectionpool.py:465)
[2022-03-09 12:03:26] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:03:26] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:03:26] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: chrome not reachable

[2022-03-09 12:03:26] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:03:26] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:03:26] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:03:26] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: chrome not reachable

[2022-03-09 12:03:26] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:03:26] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:03:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:55933/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:03:27] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:55933 (connectionpool.py:232)
[2022-03-09 12:04:31] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:55933 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:04:31] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:04:31] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:04:31] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:04:31] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:04:31] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:04:32] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:44083/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:04:32] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:44083 (connectionpool.py:232)
[2022-03-09 12:05:33] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:44083 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:05:33] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:05:33] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:05:33] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:05:33] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:05:33] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:05:34] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58727/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:05:34] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58727 (connectionpool.py:232)
[2022-03-09 12:06:36] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58727 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:06:36] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:06:36] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:06:36] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:06:36] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:06:36] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:06:38] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:56557/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:06:38] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:56557 (connectionpool.py:232)
[2022-03-09 12:07:41] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:56557 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:07:41] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:07:42] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:07:42] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:07:43] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:07:43] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:07:47] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:47161/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:08:03] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:47161 (connectionpool.py:232)
[2022-03-09 12:09:07] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:47161 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:09:07] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:09:08] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:09:08] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:09:08] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:09:08] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:09:12] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:36759/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:09:12] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:36759 (connectionpool.py:232)
[2022-03-09 12:10:16] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:36759 "POST /session HTTP/1.1" 500 736 (connectionpool.py:465)
[2022-03-09 12:10:16] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:10:16] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:10:16] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: DevToolsActivePort file doesn't exist

[2022-03-09 12:10:17] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:10:17] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:10:19] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:58879/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:10:19] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:58879 (connectionpool.py:232)
[2022-03-09 12:11:27] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:58879 "POST /session HTTP/1.1" 500 746 (connectionpool.py:465)
[2022-03-09 12:11:27] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:11:27] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:11:27] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: chrome not reachable

[2022-03-09 12:11:27] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:11:27] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:11:29] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:40487/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:11:29] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:40487 (connectionpool.py:232)
[2022-03-09 12:13:08] [   DEBUG] [urllib3.connectionpool ] - http://127.0.0.1:40487 "POST /session HTTP/1.1" 500 746 (connectionpool.py:465)
[2022-03-09 12:13:08] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - Finished Request (remote_connection.py:440)
[2022-03-09 12:13:09] [CRITICAL] [   twisted ] - Unhandled error in Deferred: (_legacy.py:147)
[2022-03-09 12:13:09] [CRITICAL] [   twisted ] -  (_legacy.py:147)
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 86, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/crawler.py", line 98, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/code/TextProcessorScrapy/spiders/Baidu.py", line 45, in __init__
    self.browser = webdriver.Chrome(options=options)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 81, in __init__
    desired_capabilities=desired_capabilities)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 157, in __init__
    self.start_session(capabilities, browser_profile)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 252, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: chrome not reachable

[2022-03-09 12:13:10] [    INFO] [scrapy.crawler ] - Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 1,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2022_3_9_1.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders'],
 'TELNETCONSOLE_ENABLED': False} (crawler.py:60)
[2022-03-09 12:13:10] [    INFO] [scrapy.middleware ] - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats'] (middleware.py:48)
[2022-03-09 12:13:11] [   DEBUG] [selenium.webdriver.remote.remote_connection ] - POST http://127.0.0.1:37187/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--no-sandbox", "--disable-dev-shm-usage", "--headless"]}}} (remote_connection.py:388)
[2022-03-09 12:13:11] [   DEBUG] [urllib3.connectionpool ] - Starting new HTTP connection (1): 127.0.0.1:37187 (connectionpool.py:232)

2021-10-26 13:26:53 [scrapy.extensions.telnet] INFO: Telnet Password: e3981fed51bd09ef
2021-10-26 13:26:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:26:53 [werkzeug] WARNING:  * Debugger is active!
2021-10-26 13:26:53 [werkzeug] INFO:  * Debugger PIN: 835-249-238
2021-10-26 13:26:53 [werkzeug] INFO:  * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)
2021-10-26 13:28:57 [werkzeug] INFO:  * Detected change in 'E:\\workspace\\TextProcessor\\TextProcessorScrapy\\settings.py', reloading
2021-10-26 13:28:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:28:57 [scrapy.extensions.telnet] INFO: Telnet Password: bb6a20e269f2e55b
2021-10-26 13:28:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:28:57 [werkzeug] WARNING:  * Debugger is active!
2021-10-26 13:28:57 [werkzeug] INFO:  * Debugger PIN: 835-249-238
2021-10-26 13:28:57 [werkzeug] INFO:  * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)
2021-10-26 13:29:08 [werkzeug] INFO:  * Detected change in 'E:\\workspace\\TextProcessor\\TextProcessorScrapy\\settings.py', reloading
2021-10-26 13:29:08 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:08 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 89, in crawl
    self.engine = self._create_engine()
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 103, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\misc.py", line 50, in load_object
    mod = import_module(module)
  File "D:\Anaconda\envs\HS\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 23, in <module>
    app.run(host='0.0.0.0', port=8080, debug=True)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\serving.py", line 1050, in run_simple
    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 337, in run_with_reloader
    reloader.run()
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 213, in run
    self.trigger_reload(filename)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 189, in trigger_reload
    sys.exit(3)
SystemExit: 3
2021-10-26 13:29:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:08 [scrapy.extensions.telnet] INFO: Telnet Password: 3f5c7f2aa4e1e47b
2021-10-26 13:29:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:08 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:7310/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}
2021-10-26 13:29:08 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1
2021-10-26 13:29:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:7310 "POST /session HTTP/1.1" 200 783
2021-10-26 13:29:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:29:09 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:7310/session/7b20fc5615edf67c0ea575fef9e494a8/timeouts {"implicit": 1000}
2021-10-26 13:29:09 [urllib3.connectionpool] DEBUG: http://127.0.0.1:7310 "POST /session/7b20fc5615edf67c0ea575fef9e494a8/timeouts HTTP/1.1" 200 14
2021-10-26 13:29:09 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:29:09 [werkzeug] WARNING:  * Debugger is active!
2021-10-26 13:29:09 [werkzeug] INFO:  * Debugger PIN: 835-249-238
2021-10-26 13:29:09 [werkzeug] INFO:  * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 89, in crawl
    self.engine = self._create_engine()
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\crawler.py", line 103, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\core\downloader\__init__.py", line 83, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 53, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Users\adm\AppData\Roaming\Python\Python37\site-packages\scrapy\utils\misc.py", line 50, in load_object
    mod = import_module(module)
  File "D:\Anaconda\envs\HS\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 23, in <module>
    app.run(host='0.0.0.0', port=8080, debug=True)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 990, in run
    run_simple(host, port, self, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\serving.py", line 1050, in run_simple
    run_with_reloader(inner, extra_files, reloader_interval, reloader_type)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 337, in run_with_reloader
    reloader.run()
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 213, in run
    self.trigger_reload(filename)
  File "D:\Anaconda\envs\HS\lib\site-packages\werkzeug\_reloader.py", line 189, in trigger_reload
    sys.exit(3)
SystemExit: 3
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.BaiduPipeline']
2021-10-26 13:29:13 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:13 [baidu] INFO: Spider opened: baidu
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-10-26 13:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 2b9815519f25f408
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.TiexuePipeline']
2021-10-26 13:29:13 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:13 [tiexue] INFO: Spider opened: tiexue
2021-10-26 13:29:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2021-10-26 13:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: a7f3b44b22ffc4fe
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.AiaaPipeline']
2021-10-26 13:29:13 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:13 [aiaa] INFO: Spider opened: aiaa
2021-10-26 13:29:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2021-10-26 13:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 7ce1f15894c707d3
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.HsNasaPipeline']
2021-10-26 13:29:13 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:13 [nasa] INFO: Spider opened: nasa
2021-10-26 13:29:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2021-10-26 13:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 995d44230af65061
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.WikiPipeline']
2021-10-26 13:29:13 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:13 [wiki] INFO: Spider opened: wiki
2021-10-26 13:29:13 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:13 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2021-10-26 13:29:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:13 [scrapy.extensions.telnet] INFO: Telnet Password: 0354b4b95acfae2c
2021-10-26 13:29:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:14 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:9945/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "platformName": "any", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}, "desiredCapabilities": {"browserName": "chrome", "version": "", "platform": "ANY", "goog:chromeOptions": {"prefs": {"profile.default_content_setting_values": {"notifications": 2, "images": 2}}, "extensions": [], "args": ["--headless"]}}}
2021-10-26 13:29:14 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1
2021-10-26 13:29:14 [urllib3.connectionpool] DEBUG: http://127.0.0.1:9945 "POST /session HTTP/1.1" 200 784
2021-10-26 13:29:14 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:29:14 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:9945/session/34e0167d21b8d3f38d98567d23e53b45/timeouts {"implicit": 1000}
2021-10-26 13:29:14 [urllib3.connectionpool] DEBUG: http://127.0.0.1:9945 "POST /session/34e0167d21b8d3f38d98567d23e53b45/timeouts HTTP/1.1" 200 14
2021-10-26 13:29:14 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.BaiduPipeline']
2021-10-26 13:29:14 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:14 [baidu] INFO: Spider opened: baidu
2021-10-26 13:29:14 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:14 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6028
2021-10-26 13:29:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5765d641b1eda630
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.TiexuePipeline']
2021-10-26 13:29:14 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:14 [tiexue] INFO: Spider opened: tiexue
2021-10-26 13:29:14 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:14 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029
2021-10-26 13:29:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'TextProcessorScrapy',
 'COMMANDS_MODULE': 'TextProcessorScrapy.commands',
 'CONCURRENT_REQUESTS': 8,
 'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
 'CONCURRENT_REQUESTS_PER_IP': 128,
 'LOG_FILE': 'log/scrapy_2021_10_26_3.log',
 'NEWSPIDER_MODULE': 'TextProcessorScrapy.spiders',
 'REACTOR_THREADPOOL_MAXSIZE': 20,
 'SPIDER_MODULES': ['TextProcessorScrapy.spiders']}
2021-10-26 13:29:14 [scrapy.extensions.telnet] INFO: Telnet Password: e6a098cd32da4457
2021-10-26 13:29:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-10-26 13:29:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'TextProcessorScrapy.middlewares.RandomUserAgentMiddleware',
 'TextProcessorScrapy.middlewares.TextCrawlDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-26 13:29:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-26 13:29:15 [scrapy.middleware] INFO: Enabled item pipelines:
['TextProcessorScrapy.pipelines.AiaaPipeline']
2021-10-26 13:29:15 [scrapy.core.engine] INFO: Spider opened
2021-10-26 13:29:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-26 13:29:15 [aiaa] INFO: Spider opened: aiaa
2021-10-26 13:29:15 [twisted] CRITICAL: Unhandled error in Deferred:
2021-10-26 13:29:15 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\Anaconda\envs\HS\lib\site-packages\twisted\internet\defer.py", line 191, in maybeDeferred
    result = f(*args, **kwargs)
  File "E:\workspace\TextProcessor\TextProcessorScrapy\middlewares.py", line 132, in collect
    @app.route('/{}'.format(str(COUNT)))
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1315, in decorator
    self.add_url_rule(rule, endpoint, f, **options)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 98, in wrapper_func
    return f(self, *args, **kwargs)
  File "D:\Anaconda\envs\HS\lib\site-packages\flask\app.py", line 1284, in add_url_rule
    "existing endpoint function: %s" % endpoint
AssertionError: View function mapping is overwriting an existing endpoint function: pushData
2021-10-26 13:29:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6030
2021-10-26 13:29:15 [TextProcessorScrapy.spiders.Aiaa] INFO: Aiaa Spider Starting!
2021-10-26 13:29:15 [TextProcessorScrapy.spiders.Aiaa] INFO: Aiaa Spider Starting!
